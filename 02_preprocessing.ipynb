{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2022-nlp/blob/main/02_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIVXAmw_tMRv"
      },
      "source": [
        "# **テキストデータの扱い方：基本中の基本編**\n",
        "\n",
        "* テキストデータは、長い長い文字列。\n",
        "* 長い長い文字列のままでは、普通は分析できない。\n",
        "* 今回は、自然言語処理における基本的な前処理について学ぶ。\n",
        "* 今回は、英語データのみを扱う。\n",
        " * 日本語データは、次回。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgeSwPsGJFWj"
      },
      "source": [
        "## str型のメソッドによる前処理\n",
        "\n",
        "* 例えば、大文字と小文字の間の変換などが実行できる。\n",
        "\n",
        " * 問：元のテキストにあった大文字と小文字の区別を無くしてしまうことのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQp382lJJFWp"
      },
      "source": [
        "text = 'The quick brown fox jumped over The Big Dog'\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaAwb7HZJFWz"
      },
      "source": [
        "text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihX9LwVuJFW4"
      },
      "source": [
        "text.upper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19LzVfl7Tcje"
      },
      "source": [
        "* 各トークンの一文字目を大文字にする。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U24TBZ82JFW8"
      },
      "source": [
        "text.title()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtCf5TIaJpEr"
      },
      "source": [
        "## NLTK\n",
        "\n",
        "* NLTKは、Pythonの有名な自然言語処理ライブラリ。2001年スタートらしい。\n",
        "\n",
        "* https://www.nltk.org/\n",
        " * WordNetについては『IT Text 自然言語処理の基礎』3.2.2(a)を参照。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceSG71XiJoka"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3GzHq46JFW_"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "* 文に分ける、単語に分ける、など、長い文字列としての言語データをより小さな単位へと分割することを、一般にtokenizationと言う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFm7LXKmnoS9"
      },
      "source": [
        "* Pythonの文字列は、複数行にわたっていても、丸括弧でくくれば一つの長い文字列になる。\n",
        " * ただし、最後の行を除いて、末尾に空白を入れておくのを忘れないように。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIiPr5JBJFXA"
      },
      "source": [
        "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
        "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
        "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
        "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "               \"which reportedly take up the size of two tennis courts.\")\n",
        "sample_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqUvs2QjoAy-"
      },
      "source": [
        "* 文ごとにtokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2m8nEPmJFXD"
      },
      "source": [
        "nltk.sent_tokenize(sample_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VDN_9m_oD7E"
      },
      "source": [
        "* 単語ごとにtokenize\n",
        " * 問：この`nltk.word_tokenize`によるword tokenizationのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjVNIwLoJFXG"
      },
      "source": [
        "print(nltk.word_tokenize(sample_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D25190-Ft9Ls"
      },
      "source": [
        "## spaCy\n",
        "\n",
        "* spaCyも、Pythonの有名な自然言語処理ライブラリ。2015年スタートらしい。\n",
        "\n",
        "* https://spacy.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS5twUUnuIPf"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjhORAuPJFXL"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6LA_YHJFXN"
      },
      "source": [
        "text_spacy = nlp(sample_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_nN_l3zQwa"
      },
      "source": [
        "[obj.text for obj in text_spacy.sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNVKKkAtx_lx"
      },
      "source": [
        "text_spacy.sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR2DcOE18LfI"
      },
      "source": [
        "* 問： 下のword tokenizationは、先ほどの`nltk.word_tokenize`によるword tokenizationとどう違うか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBuAHdR8JFXQ"
      },
      "source": [
        "print([obj.text for obj in text_spacy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens1 = list(nltk.word_tokenize(sample_text))\n",
        "tokens2 = list(text_spacy)"
      ],
      "metadata": {
        "id": "O3IdVxSmqZqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens1), len(tokens2))"
      ],
      "metadata": {
        "id": "rZBbsLq6qiXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhxnJkIsJFXS"
      },
      "source": [
        "## HTML文書\n",
        "\n",
        "* __`<p>`__や__`<a>`__や__`<div>`__など、頻繁に使うHTMLタグは頭に入れておいてください。\n",
        "\n",
        "* なぜなら、ある程度HTMLタグが読めてはじめて、スクレイピングのコードを書くための、HTMLソースの下調べができるからです。\n",
        " * 自前でWeb上から分析対象のテキストデータを取得するときは、ダウンロードしようとするWebページのHTMLの構造を自分の目で確認する。\n",
        "\n",
        "* 問：誰かによって整備されたデータセットではなく、自前でHTML文書をスクレイピングすることのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ewyVIu3auUl"
      },
      "source": [
        "### HTML文書のダウンロード\n",
        "* いくつか方法はあるが、ここではrequestsモジュールを使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3qV1WOpJFXT"
      },
      "source": [
        "import requests\n",
        "\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.text\n",
        "print(content[2745:3948])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vte11lpuXV_"
      },
      "source": [
        "### Beautiful Soupの利用\n",
        "\n",
        "* HTML文書の構造を解析するためによく使われるライブラリ。\n",
        "\n",
        "* 参考資料：「Beautiful Soup 4によるスクレイピングの基礎」\n",
        "\n",
        " * https://www.atmarkit.co.jp/ait/articles/1910/18/news015.html\n",
        "\n",
        "* 注意： 自動巡回やスクレイピングを禁止しているWebサイトもあるので注意しよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* HTMLタグを取り除くコードの例"
      ],
      "metadata": {
        "id": "N5IM18XyrU1Q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6UAz3mjJFXY"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    # 下の正規表現の意味を説明してみよう。\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "clean_content = strip_html_tags(content)\n",
        "print(clean_content[1163:1957])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au1nuTllwX07"
      },
      "source": [
        "## 演習\n",
        "* 上の`clean_content`を単語に分割し、各単語の出現頻度を求め、出現頻度の高い順に上位100の単語を、出現頻度とともに表示しよう。\n",
        "* `clean_content`の内容を、すべて小文字に変換した後で、同じことをしてみよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdGJ9gYO_j1o"
      },
      "source": [
        "# 演習1の答案\n",
        "text_spacy = nlp(clean_content)\n",
        "tokens = [obj.text for obj in text_spacy]\n",
        "print(tokens[10000:10020])\n",
        "# 以下続けてコードを書く。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBlgeuCY1YPX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTwDPU2fx0i0"
      },
      "source": [
        "## 様々な前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj8CyGmPJFXr"
      },
      "source": [
        "### 特殊文字、数字、記号の除去\n",
        "\n",
        "* reモジュールを使う。reはregular expression(正規表現)のこと。\n",
        "\n",
        "* 問：テキストデータの前処理において特殊文字、数字、記号などを除去することのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wThZGxr4oTFT"
      },
      "source": [
        "* 問：下のセルで使われている２つの正規表現はそれぞれどういう意味か？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dkc4ESDJFXs"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUwKvQ-1JFXx"
      },
      "source": [
        "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy9x4XFyJFYL"
      },
      "source": [
        "remove_special_characters(s, remove_digits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2vT0GK5JFYQ"
      },
      "source": [
        "remove_special_characters(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeUHPmhDJFZC"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "* 語尾が変化する単語の、その変化を無くして、語幹を得る。\n",
        "* 得られる語幹は、英単語として通用しない文字列になることが多い。\n",
        "* Stemming and Lemmatization in Python\n",
        " * https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
        "\n",
        "\n",
        "* 問：テキストデータの前処理としてstemmingをすることのメリットとデメリットは何か？\n",
        "* 問：様々な種類のstemmerがあるのはなぜか？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKIclu60ob5A"
      },
      "source": [
        "* Porter Stemmerを使ってみる （stemmerと言えばこれ、というぐらい良く知られている。）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ndJ4XOKJFZD"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmWLISH-JFZG"
      },
      "source": [
        "ps.stem('lying')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7KRj1jtJFZJ"
      },
      "source": [
        "ps.stem('strange')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgQJp2SH7OC_"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "* 動詞や形容詞は原型に、名詞は単数形に、等と、単語の元々の形に直すこと。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2ExlFqJFaw"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga-E47JKJFaz"
      },
      "source": [
        "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb-PrIeqJFa5"
      },
      "source": [
        "spacy_lemmatize_text(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsKAXlvJFa7"
      },
      "source": [
        "## ストップワード\n",
        "\n",
        "* ストップワードとは、非常に頻繁に使われるため、言語データの分析にあまり役に立ちそうにない単語のこと。\n",
        "* これこそが英語のストップワードだ！と言えるような決定的なストップワードのリストがあるわけではない。\n",
        " * 主要なNLPライブラリでは、あらかじめ用意されたストップワードのリストを使うことができる。\n",
        " * しかし、分析したいテキストデータに合わせて、ストップワードのリストをカスタマイズすることも、よくある。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TaYuRW2AxvE"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "print(STOP_WORDS)\n",
        "print(len(STOP_WORDS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkJLKKxrJFa7"
      },
      "source": [
        "def remove_stopwords(text, stopwords=None):\n",
        "    if not stopwords:\n",
        "        stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "    tokens = [obj.text for obj in nlp(text)]\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycusSsPBJFbA"
      },
      "source": [
        "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWKjTPnzJFbD"
      },
      "source": [
        "remove_stopwords(spacy_lemmatize_text(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVfSpyrrCJ63"
      },
      "source": [
        "## 現代的なトークン化\n",
        "* 図表は下記のブログ記事より。\n",
        " * https://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html\n",
        "\n",
        "![Segmentation.png](https://raw.githubusercontent.com/tomonari-masada/course2022-nlp/main/Segmentation.png)\n",
        "![inherent_task_complexity.png](https://raw.githubusercontent.com/tomonari-masada/course2022-nlp/main/image3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDgoJvYe-hK1"
      },
      "source": [
        "* 今は、tokenizationと言えば、ほぼ、サブワードに分けることを意味する。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "HzdnWpGYuybH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "0vTmXBVYvAjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('This framework generates embeddings for each input sentence')"
      ],
      "metadata": {
        "id": "1s3SWJGIvCp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj8RJ6EoCwgp"
      },
      "source": [
        "# 課題2\n",
        "\n",
        "* Wikipediaの適当な英語のエントリをダウンロードする。\n",
        "\n",
        " * 選ぶのが面倒という方はAIのエントリでもどうぞ。\n",
        "   * https://en.wikipedia.org/wiki/Artificial_intelligence\n",
        "\n",
        "* Beautiful Soupで本文のテキストだけを取得する。\n",
        "\n",
        " * HTMLのソースを見て、どこが本文かを確認する。\n",
        " * あるいは、ネット検索をして、Wikipediaのエントリから本文だけを取得する方法を調べる。\n",
        "\n",
        "* 以下の前処理をする。\n",
        "\n",
        " * 大文字は小文字にする。\n",
        "\n",
        " * ストップワードを除去する。\n",
        "\n",
        " * lemmatizeする。\n",
        "\n",
        "* 各単語の出現回数を求め、表示する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDMb9o6cgMg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}