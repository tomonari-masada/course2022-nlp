{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYjKx2nqsB3ffJ2P9o1uCI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2022-nlp/blob/main/03_elementary_Japanese_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_GmRdAi6h89"
      },
      "source": [
        "# **日本語データの扱い方**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4X6Ihrby4V5"
      },
      "source": [
        "* 日本語のテキストは空白によって単語に分たれていない。\n",
        " * 同じように、テキストが空白によって単語へ分たれていない言語は？\n",
        "* そのため、まず最初にテキストを単語へ分割する必要がある。\n",
        "* この作業を**形態素解析**と言う。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 長い文字列としてのテキストを、より細かい単位へ分割することを、一般にtokenizationと言う。\n",
        "* tokenizationの単位は、単語のように意味的なまとまりを持つ単位とは限らない。\n",
        " * サブワードは、意味的なまとまりを持たない（単独のサブワードを見ただけでは意味が分からないことが多い）。"
      ],
      "metadata": {
        "id": "6Ys1pMQo5AUt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrnUvkLz8i2y"
      },
      "source": [
        "## spaCyで形態素解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vALkrVWnUbq4"
      },
      "source": [
        "* ここでは、spaCyを介してSudachiという形態素解析器を使う。\n",
        " * 形態素解析器としては、他には、MeCabやJUMANが有名。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy[ja]"
      ],
      "metadata": {
        "id": "MppUnSSgw8rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* spaCyの日本語版をインストールすると、sudachiがインストールされる。\n",
        " * 以下のように、コマンドラインからでも使えるようになっている。"
      ],
      "metadata": {
        "id": "Wn1v7xV9xNGW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_mUBCBAgz1o"
      },
      "source": [
        "!echo \"すもももももももものうち\" | sudachipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mREm_MVK8m5q"
      },
      "source": [
        "### spaCyの日本語統計モデルをロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzXLkIsEerem"
      },
      "source": [
        "from spacy.lang.ja import Japanese\n",
        "\n",
        "nlp = Japanese()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jW0p4D3_sk8"
      },
      "source": [
        "### spaCyからSudachiを利用\n",
        "* 分かち書きされた単語そのもの以外の、品詞などの情報も表示させている。\n",
        " * tagに「形容動詞」は無く、その語幹が「形状詞」とされている。\n",
        " * 「じゃない」の「ない」は、tagでは「形容詞」だが、posでは「AUX」である。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbVAM7qvgW0n"
      },
      "source": [
        "doc = nlp('吾輩は猫である。名前はまだ無い。')\n",
        "print(doc.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNu4wuDP_qU-"
      },
      "source": [
        "for token in doc:\n",
        "  print(f'text:{token.text}, pos:{token.pos_}, tag:{token.tag_}, lemma:{token.lemma_}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpOpSOjEBNWk"
      },
      "source": [
        "## Mecab+NEologdで形態素解析\n",
        "* MeCabは日本語の形態素解析ツール。有名。\n",
        "* [NEologd](https://github.com/neologd/mecab-ipadic-neologd)はneologisms（新表現）にも対応した日本語の辞書。\n",
        " * 最近は更新されていないらしい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAv7FNfdBdym"
      },
      "source": [
        "### MeCabのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2CLBXEUBI_n"
      },
      "source": [
        "!apt-get install mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYCZjmO4BmJU"
      },
      "source": [
        "### NEologdのインストール"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `file`というコマンドをインストールしておく。"
      ],
      "metadata": {
        "id": "x0z7BC521VL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install file"
      ],
      "metadata": {
        "id": "CS-lmClC08I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 以下のセルの実行が必要（かもしれない）。"
      ],
      "metadata": {
        "id": "KZjUXEjp1uRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801\n",
        "!rm /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801.tar.gz"
      ],
      "metadata": {
        "id": "PhU-zrKx1mZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aANstIJBZYv"
      },
      "source": [
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null \n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWtwJCGLDYxn"
      },
      "source": [
        "* NEologdがどこにインストールされたかを調べる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h6Agk1NC2cN"
      },
      "source": [
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `mecabrc`というファイルへのパスが後で必要になるので、調べておく。"
      ],
      "metadata": {
        "id": "oIQ5lwb5zFaJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcNJjr2zEEGe"
      },
      "source": [
        "!find / -iname mecabrc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl4XYhRvDjzi"
      },
      "source": [
        "### mecabコマンドによる形態素解析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_neN9ZvDc6w"
      },
      "source": [
        "!echo \"すもももももももものうち\" | mecab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VoocfU4N3Od"
      },
      "source": [
        "!echo \"人工知能科学研究科\" | mecab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95LQ30G0N-Tc"
      },
      "source": [
        "!echo \"人工知能科学研究科\" | mecab -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35d7gkFVGRwn"
      },
      "source": [
        "### PythonからMeCabを利用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBIN1pRRP-A-"
      },
      "source": [
        "!pip install mecab-python3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkJ628O0Q1xy"
      },
      "source": [
        "* mecabrcへのパスを`-r`オプションで指定してからMeCabを使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iczlghHvES2N"
      },
      "source": [
        "import os\n",
        "import MeCab\n",
        "\n",
        "mecab = MeCab.Tagger('-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd -r /etc/mecabrc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy4uuA2sEZM6"
      },
      "source": [
        "node = mecab.parseToNode(\"吾輩は猫である。名前はまだ無い。\")\n",
        "while node:\n",
        "  print(f'{node.surface}\\t{node.feature}')\n",
        "  node = node.next"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 品詞だけ取り出す"
      ],
      "metadata": {
        "id": "aryyYNMQLEG9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGY2437EFaBk"
      },
      "source": [
        "node = mecab.parseToNode(\"吾輩は猫である。名前はまだ無い。\")\n",
        "while node:\n",
        "  features = node.feature.split(',')\n",
        "  word = features[6]\n",
        "  if word == '*':\n",
        "    word = node.surface\n",
        "  if len(word):\n",
        "    print(f'{word}\\t{features[0]}')\n",
        "  node = node.next"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tME6lbWjErVS"
      },
      "source": [
        "# 課題３\n",
        "\n",
        "* （この課題は、宿題ではなく、いまここで作業しつつ解いてしまいます。）\n",
        "* Wikipediaの複数の記事を、lemmaを半角スペースでつないだ、長い文字列へ変換する。\n",
        " * ここでは、コンピュータ科学の様々な分野の記事を題材として使う。\n",
        "* scikit-learnの`TfidfVectorizer`を使って、各記事における単語の出現頻度からなる文書ベクトルを得る。\n",
        "* 特徴ベクトルどうしの類似度を計算し、「人工知能」分野と最も似ている順に　３つの分野がどの分野かを求める。\n",
        " * 答えは自分の感覚でチェック。\n",
        " * 文書ベクトルを作る時に、単語の品詞を名詞に限定するなど、品詞の情報を使うことで結果を改善できるかどうかも、余裕があれば試行錯誤する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n4zl1W407u6"
      },
      "source": [
        "## 課題の手順(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhCnUa1syjI5"
      },
      "source": [
        "### Wikipediaのエントリをダウンロードして形態素解析を適用\n",
        "\n",
        "* Wikipediaの「人工知能」エントリをダウンロードする。\n",
        " * https://ja.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD\n",
        "* そして、そこに含まれる段落（__`<p>`__タグで囲まれた範囲）を列挙する。\n",
        "* 各段落のテキストに形態素解析を適用する。\n",
        "* 形態素解析で得られたlemmaを半角スペースでつないで、エントリ全体をひとつの長い文字列にする。\n",
        " * text、つまり、単語に分たれたそのままの文字列を半角スペースでつなぐのではない。\n",
        " * lemma（原型に戻したもの）を半角スペースでつなぐこと。\n",
        " * posやtagを見て、不要そうな単語を適当に削除してもよい。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 「人工知能」エントリをダウンロードしてparserのインスタンスを作る。"
      ],
      "metadata": {
        "id": "81G6ObuH67vC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ2gPVtnDZvk"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "\n",
        "url = 'https://ja.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD'\n",
        "html = urlopen(url) \n",
        "soup = BeautifulSoup(html, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 段落のテキストを取得する。"
      ],
      "metadata": {
        "id": "bLDpXnsc7BrD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y5MSWBXDuE-"
      },
      "source": [
        "lines = list()\n",
        "for para in soup.find_all('p'):\n",
        "  lines.append(para.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CafzVR3NHG0c"
      },
      "source": [
        "lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Sudachiで形態素解析し、分かち書き後のlemmaを取得する。"
      ],
      "metadata": {
        "id": "6OmdyLES7EM2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXP8_h7v-Yb"
      },
      "source": [
        "x_pos = ['SPACE', 'PUNCT', 'AUX', 'ADP', 'SYM', 'DET', 'SCONJ', 'PART'] # 除去する品詞\n",
        "tokens = list()\n",
        "for line in lines[:10]:\n",
        "  for token in nlp(line):\n",
        "    pos = token.pos_\n",
        "    if not pos in x_pos:\n",
        "      print(f'text:{token.text}, pos:{token.pos_}, tag:{token.tag_}, lemma:{token.lemma_}')\n",
        "      tokens.append(token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* すべてのlemmaを半角スペースでつないで、長い文字列にする。"
      ],
      "metadata": {
        "id": "QQGs_JNu7LaZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4dxDREcwZOi"
      },
      "source": [
        "doc_AI = ' '.join(tokens)\n",
        "print(doc_AI)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M94JaaA1zWRB"
      },
      "source": [
        "* 上記の操作をまとめておこなう関数を定義しておく。\n",
        " * 後で、各エントリについて、この関数を呼び出す。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAJ5fUnTzV7w"
      },
      "source": [
        "def morph(soup, nlp):\n",
        "\n",
        "  lines = list()\n",
        "  for para in soup.find_all('p'):\n",
        "    lines.append(para.text)\n",
        "\n",
        "  x_pos = ['SPACE', 'PUNCT', 'AUX', 'ADP', 'SYM', 'DET', 'SCONJ', 'PART']\n",
        "  tokens = list()\n",
        "  for line in lines:\n",
        "    for token in nlp(line):\n",
        "      pos = token.pos_\n",
        "      if pos not in x_pos:\n",
        "        tokens.append(token.lemma_)\n",
        "\n",
        "  return ' '.join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uha2LvYf1LZd"
      },
      "source": [
        "## 課題の手順(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAByKozFtYwF"
      },
      "source": [
        "### 「コンピュータ科学」の様々な分野に関するWikipediaエントリについて上記の作業を実行\n",
        "*  「人工知能」エントリの下部にある「コンピュータ科学」の分野一覧から、aタグのhref属性にあるURLを抜き出す。\n",
        "* ただし、__`/wiki/`__という文字列で始まっているURLであり、かつ、テンプレートの状態でないものだけを抜き出す。\n",
        " * Wikipediaのクローリングについてもっと詳しく知りたい場合は、下記のページ等を参照されたい。\n",
        "   * https://medium.com/@robinlphood/tutorial-a-simple-crawler-for-wikipedia-d7b6f6f55d5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDNPdPZ5K_1B"
      },
      "source": [
        "* 「人工知能」エントリの下部にある「コンピュータ科学」の分野一覧から、aタグのhref属性にあるURLを抜き出す。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qTCAg07qwcn"
      },
      "source": [
        "target_str = '表話編歴コンピュータ科学'\n",
        "prefix = '/wiki/'\n",
        "\n",
        "urls = dict()\n",
        "for table in soup.find_all('table'):\n",
        "  if table.text[:len(target_str)] != target_str: continue\n",
        "  for td in table.find_all('td'):\n",
        "    for a in td.find_all('a'):\n",
        "      if not a.text: continue\n",
        "      try:\n",
        "        if a.text.find('英語版') == -1:\n",
        "          href = a['href']\n",
        "          if href[:len(prefix)] == prefix and href.find('/Template:') == -1 and href.find('/Category:') == -1:\n",
        "            urls[a.text] = 'https://ja.wikipedia.org' + href\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "for k in urls:\n",
        "  print(k, urls[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hZNPDG-LFJc"
      },
      "source": [
        "* 各エントリをダウンロードしてparserのインスタンスを作り、辞書として保存。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luy0M3LjBd63"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "soups = dict()\n",
        "for k in tqdm(urls):\n",
        "  html = urlopen(urls[k]) \n",
        "  soups[k] = BeautifulSoup(html, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AslsPUYaLHer"
      },
      "source": [
        "* 先ほど定義した関数morph()を使って各エントリを形態素解析し、lemmaが半角スペースで区切られた文字列へ変換する。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_FMR37hvTHZ"
      },
      "source": [
        "genre = list()\n",
        "corpus = list()\n",
        "for k in tqdm(soups):\n",
        "  genre.append(k)\n",
        "  doc = morph(soups[k], nlp)\n",
        "  corpus.append(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO3akgZiLKIJ"
      },
      "source": [
        "* 再利用するために、csvファイルとして保存しておく。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvZKoPJXNNgO"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "genre.append('人工知能')\n",
        "corpus.append(doc_AI)\n",
        "\n",
        "df = pd.DataFrame(list(zip(genre, corpus)), columns=['genre', 'text'])\n",
        "print(df.head())\n",
        "df.to_csv('cs_corpus.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaIiKbJu1Ps9"
      },
      "source": [
        "## 課題の手順(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnSE8kLf1Re6"
      },
      "source": [
        "### scikit-learnのTfidfVectorizerで単語の出現頻度を要素とするベクトルを作成\n",
        "* これにより、各文書のベクトル表現が得られる。\n",
        "* 興味のある対象のベクトル表現を得ることは、その対象を機械学習アルゴリズムの入力データとして使うための第一歩。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 先ほど作成したcsvファイルを読んで、textカラムをTfidfVectorizerのインスタンスでベクトル化する。\n",
        " * TfidfVectorizerのmin_dfやmax_dfをチューニングすると、より面白い結果になる可能性あり。"
      ],
      "metadata": {
        "id": "FQAbDoSN4Hv_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbmW1-x5zs99"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "df = pd.read_csv('cs_corpus.csv')\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['text']).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9HpWKob5BSi"
      },
      "source": [
        "print('文書数:{}; 語彙サイズ：{}'.format(*X.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 語彙を取得"
      ],
      "metadata": {
        "id": "oukTR5yb4Vya"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLut43Mj1jDh"
      },
      "source": [
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* TF-IDF値を全文書にわたって和をとった値の大きい順で上位20単語を見てみる\n",
        " * コーパス全体で重要な単語を見ていることになる。"
      ],
      "metadata": {
        "id": "6Ghe4W7o4YRT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Mcat9s16Gk"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "vocab = np.array(vocab)\n",
        "print(vocab[np.argsort(- X.sum(axis=0))][:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 「人工知能」エントリと他のエントリとの距離を求める。\n",
        " * 注：scipy.spatial.distance.cosineは、cosine類似度を1から引いたもの。"
      ],
      "metadata": {
        "id": "O3DmSOL04bFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genre = df['genre'].values.tolist()\n",
        "print(', '.join(genre))"
      ],
      "metadata": {
        "id": "6cJxj18n44Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-iVfMub3Aon"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "index_AI = genre.index('人工知能')\n",
        "print(f'「{genre[index_AI]}」と「{genre[7]}」との間での・・・')\n",
        "print(f'ユークリッド距離: {np.linalg.norm(X[0] - X[index_AI])}')\n",
        "print(f'内積: {np.dot(X[0], X[index_AI])}')\n",
        "print(f'コサイン距離: {distance.cosine(X[0], X[index_AI])}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfb2TM94L-sC"
      },
      "source": [
        "### ここから各自作業。\n",
        "* 「人工知能」と、Wikipediaのエントリの類似度の上で、最も近いジャンルは、どれ？\n",
        "* ユークリッド距離とコサイン距離のうち、どちらを類似度として使うのが良さそうか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-da3ID_L0nK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}