{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_bag_of_words.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKsyFWg4c3iQduVKAQvFCr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2022-nlp/blob/main/01_bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzUPIsj03QUI"
      },
      "source": [
        "# **bag-of-wordsモデル**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xPPAciVN-m-"
      },
      "source": [
        "* 本日の参考書\n",
        " * C. D. Manning, P. Raghavan & H. Schütze. Introduction to Information Retrieval. © 2008 Cambridge University Press https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKEP5r0nOxjd"
      },
      "source": [
        "* 軽い前置き\n",
        " * この授業では、Pythonのコーディングの基礎は習得済みであることを前提します。\n",
        " * また、NumPyやscikit-learnの基本的な使い方は習得済みであることを前提します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiozJha53etL"
      },
      "source": [
        "## 歴史：bag-of-wordsモデルを取り巻く状況\n",
        "* bag-of-wordsは、文書をモデル化する方法の、一つ。\n",
        "* 今は、EDA (exploratory data analysis) 以外ではあまり使われない。\n",
        "* このbag-of-wordsから、現在のcontextualized word embeddingsに至るまでの流れを把握することが、この授業における学習の目標。\n",
        " * LLMs (large language models) は、contextualized word embeddingsを実現する手法の一つ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba4ndKX4IJke"
      },
      "source": [
        "### 用語\n",
        "* 「単語トークン word token」（あるいは単に「トークン token」）\n",
        " * 単語の一回一回の出現のこと。\n",
        " * このセルで「この」という単語は5回現れている。\n",
        " * このことを、このセルでは「この」という単語のトークンが5個ある、などと言い表す。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z57oKsOVGOWf"
      },
      "source": [
        "### 単語のmultisetとしての文書\n",
        "* **bag-of-wordsモデル**とは、文書をベクトルとしてモデル化する手法のひとつ。\n",
        " * 他にも文書をベクトル化する手法はある。\n",
        "* bag-of-wordsモデルにおいては、文書における単語トークンの**出現順序が無視される**。\n",
        "* つまり、文書を、バッグに入ったアイテムの集まりのようにモデリングする（下図参照）。\n",
        " * 言い換えれば、文書を単語の**multiset**として扱うのがbag-of-wordsモデルである。\n",
        "\n",
        "* 参考資料\n",
        " * https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/blob/master/notebooks/MLA-NLP-Lecture1-BOW.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBk-WNX4IRVb"
      },
      "source": [
        "![bag-of-words.png](https://raw.githubusercontent.com/tomonari-masada/course2022-nlp/main/bag-of-words.png)\n",
        "\n",
        "* 図は下記のWebページより。\n",
        " * https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PilALQV12kVD"
      },
      "source": [
        "### 単語のベクトル表現の隆盛\n",
        "* 最近では、言語データをモデル化するとき、単語（あるいはsubword）のベクトル表現を用いる。\n",
        "* このベクトル表現は、埋め込みembeddingと呼ばれる。\n",
        "* word embeddingやsubword embeddingを使うのが、今は主流。\n",
        " * document embeddingも、word embeddingをもとにして構成する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-5GrvauMKDq"
      },
      "source": [
        "### 今となってはobsoleteなbag-of-wordsモデル\n",
        "* 論文では今でも、baselineとして、TF-IDFやBM25など、bag-of-wordsモデルが引き合いに出されることはある。\n",
        " * 新しい手法を考え出しても、bag-of-wordsに勝てなければ意味がない、といった使い方。\n",
        "* 参考 \n",
        " * https://twitter.com/moguranosenshi/status/1306406087445196800\n",
        " * https://twitter.com/sho_yokoi/status/1553044631864360960\n",
        " * https://twitter.com/odashi_t/status/1552951268842545154\n",
        "* そのため、授業の最初に、bag-of-wordモデルについて簡単に説明しておく。\n",
        " * BM25 https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jn1BM6BJtei"
      },
      "source": [
        "### 自然言語処理の歴史\n",
        "* 興味がある方は、スタンフォード大の自然言語処理の授業が、この10年間でいかに大きく内容を変えているか、調べてみましょう。\n",
        "\n",
        " * http://web.stanford.edu/class/cs224n/index.html の\"Previous offerings\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJZFJ9B0KNQO"
      },
      "source": [
        "## binary vector\n",
        "* 最も単純には、文書は、各単語が出現するかしないかの、1/0の2値ベクトルでモデル化できる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF13j6DLLfUL"
      },
      "source": [
        "### 用語\n",
        "* 「語彙 vocabulary」\n",
        " * あるデータセットに出現する単語の集合のこと。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhf7MJe4NH3P"
      },
      "source": [
        "### scikit-learnのCountVectorizer\n",
        "* 各documentは、半角スペースでつながれた単語の列として準備しておく。\n",
        "* binary=Trueとすると、0/1の2値ベクトルが得られる。\n",
        "* インスタンスを作り、fit_transformする、という使い方は、scikit-learnにおけるデータの前処理のときと同様。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAsV0OzFGKr8"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSPMo5HnOeLK"
      },
      "source": [
        "* 文書の集合＝コーパスを用意する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JvB4acfMxQq"
      },
      "source": [
        "corpus = [\"This document is the first document.\",\n",
        "          \"This document is the second document.\",\n",
        "          \"And this is the third one.\",\n",
        "          \"Where is the fourth one?\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9piYSlbPSan"
      },
      "source": [
        "* CountVectorizerをbinary=Trueで使う"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEcGpcc4M7bo"
      },
      "source": [
        "binary_vectorizer = CountVectorizer(binary=True) # 2値ベクトルとして表現\n",
        "X = binary_vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16VoXJLcNtmW"
      },
      "source": [
        "* 文書の2値ベクトル表現の確認\n",
        " * 疎なベクトルとして得られることに注意。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nl4fjykNDo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3032dc4d-41fd-40e6-f164-fcacd6e85187"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 9)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 2)\t1\n",
            "  (1, 9)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 6)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 0)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 5)\t1\n",
            "  (3, 4)\t1\n",
            "  (3, 7)\t1\n",
            "  (3, 5)\t1\n",
            "  (3, 10)\t1\n",
            "  (3, 3)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HVn-V4OMa02",
        "outputId": "8a88e01b-294f-4b2b-b556-98ce022ef45b"
      },
      "source": [
        "type(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUICjJZ2N6px"
      },
      "source": [
        "* 疎な表現を通常のndarrayに戻すには・・・"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJYPyDPDNsIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2680538f-a901-4ba7-dc5a-d61136a3377a"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
              "       [0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0],\n",
              "       [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
              "       [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzbZQPHuOAIP"
      },
      "source": [
        "### 語彙を確認\n",
        "* 先頭の大文字は自動的に小文字に変換されていることが分かる。\n",
        "* ピリオドや疑問符は削除されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQAAI9LKN5vL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8011cd60-f1ff-445e-e955-71c396163e97"
      },
      "source": [
        "binary_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'document': 1,\n",
              " 'first': 2,\n",
              " 'fourth': 3,\n",
              " 'is': 4,\n",
              " 'one': 5,\n",
              " 'second': 6,\n",
              " 'the': 7,\n",
              " 'third': 8,\n",
              " 'this': 9,\n",
              " 'where': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC5TW6dgMhsU",
        "outputId": "d0582b61-53d8-4c23-8752-8f65d12753ae"
      },
      "source": [
        "type(binary_vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P1dFVlcN_Pd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d31dbd-55df-47db-b106-ba6968c15614"
      },
      "source": [
        "print(binary_vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'document', 'first', 'fourth', 'is', 'one', 'second', 'the', 'third', 'this', 'where']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4AeNQK2P1GW"
      },
      "source": [
        "### 新しい文書をベクトルに変換\n",
        "* sklearnでよくやるように、transformメソッドを使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egmnVXnHPbkX"
      },
      "source": [
        "new_doc = [\"This is the new document.\"]\n",
        "\n",
        "new_vectors = binary_vectorizer.transform(new_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHWGA6W4Psto"
      },
      "source": [
        "* 新出の単語は無視される点に注意\n",
        " * OoV (out-of-vocabulary) wordsの問題\n",
        " * この問題は、NLPの世界では、超重要な問題。\n",
        " * 今は、subwordの利用により、OoV問題を回避する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fXiLjojM-s_"
      },
      "source": [
        "* newという単語は、無視されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqnzGOgmPj5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb9bc37-a9ba-4ad3-b404-c8faba5bcc67"
      },
      "source": [
        "new_vectors.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTPClzcmP84l"
      },
      "source": [
        "## word count vector\n",
        "* 文書における各単語の出現回数を使って、文書のベクトル表現を得ることもできる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCBPOUUoQNoC"
      },
      "source": [
        "### scikit-learnのCountVectorizer\n",
        "* CountVectorizerをデフォルト設定で（binary=Trueとせずに）使う\n",
        "* すると、単語の出現回数による文書のベクトル表現が得られる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZNhmGTuPmo4"
      },
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "X = count_vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yzMyTrhQWmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efdf4976-c927-4e19-9ffc-da18a411c2da"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 2, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
              "       [0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0],\n",
              "       [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
              "       [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE_YpgstQfL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d0de00-2c0b-4484-c205-02bbeb7cf059"
      },
      "source": [
        "new_vectors = count_vectorizer.transform(new_doc)\n",
        "print(new_vectors.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 0 1 0 0 1 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etJEdIKK5Ejo"
      },
      "source": [
        "## TF-IDF\n",
        "* 文書をベクトル化する古典的な手法。\n",
        "* TF-IDFは、TFとIDFの積である。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZZghSTZ5Gi_"
      },
      "source": [
        "### TF (term frequency)\n",
        "* 文書に含まれる単語トークンの数（つまり、単語の出現回数の総和）を、その文書の長さと呼ぶ。\n",
        "* TFとは、各々の単語が文書のなかで出現する回数を、その文書の長さで割ったもの。\n",
        " * 文書のなかで頻出する単語ほどTFは大きくなる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7M88Lm1QwQF"
      },
      "source": [
        "### IDF (inverse document frequency)\n",
        "* IDFとは、DFの逆数。\n",
        "* DFとは、ある単語が含まれる文書の数を、総文書数で割ったものである。\n",
        " * 文書集合のなかで稀少な単語ほどIDFは大きくなる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B13HQa1G5U_j"
      },
      "source": [
        "### TF-IDF (term frequency–inverse document frequency)\n",
        "* TF-IDFは、TFとIDFの積。\n",
        "* 積を求める前に、TFのルートもしくは対数をとったり、IDFのルートもしくは対数をとったりする。\n",
        " * 大きめの値が、効きすぎないようにする。\n",
        " * 対数をとるときは、ゼロの対数をとることにならないような工夫をする。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0RtY0PtSq3D"
      },
      "source": [
        "### TF-IDFの式の例\n",
        "\n",
        "\\begin{align}\n",
        "x_{d,w} = \\frac{n_{d,w}}{n_d} \\cdot ( 1 + \\ln\\frac{m}{m_w}) \\tag{1}\n",
        "\\end{align}\n",
        "\n",
        "where \n",
        "\n",
        " * $n_{d,w}$ is the frequency of the word $w$ in the document $d$, \n",
        " * $n_d$ is defined as $n_d \\equiv \\sum_w n_{d,w}$,\n",
        " * $m_w$ is the number of documents containing the word $w$, and\n",
        " * $m$ is the total number of documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsLF9WdGHgFD"
      },
      "source": [
        "### TF-IDFの式のバリエーション\n",
        "\n",
        "![img462.png](https://raw.githubusercontent.com/tomonari-masada/course-nlp2020/master/img462.png)\n",
        "\n",
        "https://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XfakOH8HqZ9"
      },
      "source": [
        "* 式の選び方\n",
        "  * どの式の形がいいかは、downstream taskの性能をcross validationで評価して選ぶ。\n",
        "  * どんな場合でもこれが一番、という式は、ない。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9_0aZB0TOnl"
      },
      "source": [
        "### scikit-learnのTfidfVectorizer\n",
        "* scikit-learnでのTF-IDFの計算式がどうなっているかは、下記ページを参照。\n",
        " * https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KisBcoHQjn8"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygrAINVAOm2b"
      },
      "source": [
        "* デフォルトの設定を確認してみる。\n",
        " * 次のパラメータは変更していいかもしれない。\n",
        " * max_df, min_df, ngram_range, norm, smooth_idf, stop_words, sublinear_tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA_F0D0tGMKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7e883e-76fb-46c2-c00d-7cc2762dd8d3"
      },
      "source": [
        "tfidf_vectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZYEcdYaTV28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb517860-f891-4561-cc7f-9e81bd4a340b"
      },
      "source": [
        "X = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.74846041 0.47466356 0.         0.24769914 0.\n",
            "  0.         0.24769914 0.         0.3029716  0.        ]\n",
            " [0.         0.74846041 0.         0.         0.24769914 0.\n",
            "  0.47466356 0.24769914 0.         0.3029716  0.        ]\n",
            " [0.52898651 0.         0.         0.         0.2760471  0.41705904\n",
            "  0.         0.2760471  0.52898651 0.33764523 0.        ]\n",
            " [0.         0.         0.         0.56199026 0.29326983 0.44307958\n",
            "  0.         0.29326983 0.         0.         0.56199026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmsh54ziOr-m"
      },
      "source": [
        "* 文書ベクトルはL2ノルムが1となるように長さを変更されている。\n",
        " * TfidfVectorizer()のnormパラメータで変更可能。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kSbn0uyUxOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c51f227-1223-45ef-a586-85b13785d99a"
      },
      "source": [
        "import numpy as np\n",
        "np.linalg.norm(X, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPGd8r-pIAVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6061c2-2e5d-46f1-a2c5-2c6ef20d4872"
      },
      "source": [
        "tfidf_vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['and',\n",
              " 'document',\n",
              " 'first',\n",
              " 'fourth',\n",
              " 'is',\n",
              " 'one',\n",
              " 'second',\n",
              " 'the',\n",
              " 'third',\n",
              " 'this',\n",
              " 'where']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ-M-qZjTYsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a5926e-44a5-4cf9-f901-105a7e63dae2"
      },
      "source": [
        "new_vectors = tfidf_vectorizer.transform(new_doc).toarray()\n",
        "print(new_vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.6284927  0.         0.         0.41599288 0.\n",
            "  0.         0.41599288 0.         0.50881901 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--XKc4eATjf_"
      },
      "source": [
        "* 各単語のIDF\n",
        " * IDFはそれぞれの単語について一意に決まる値。\n",
        " * 文書ごとに求まる値ではない。\n",
        " * コーパスが変わると、IDFも変わる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugsq3YAtTfRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e5587e-fa7e-4948-8a0f-4d60e6b0ed67"
      },
      "source": [
        "tfidf_vectorizer.idf_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.91629073, 1.51082562, 1.91629073, 1.91629073, 1.        ,\n",
              "       1.51082562, 1.91629073, 1.        , 1.91629073, 1.22314355,\n",
              "       1.91629073])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8niqN06T-eX"
      },
      "source": [
        "## bag-of-wordsベクトルの応用\n",
        "* 文書間の類似度の計算に使える"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H1V8jSgO_Wq"
      },
      "source": [
        "* sklearnでは、ベクトルが長さ1にnormalizeされている。\n",
        "* そのため、内積がコサイン類似度に一致する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP0KvdykThdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f6b493-0513-4513-d560-9de7fd6eb31b"
      },
      "source": [
        "for i in range(4):\n",
        "  print(np.dot(X[i], new_vectors[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8306417662781156\n",
            "0.8306417662781156\n",
            "0.401467570331823\n",
            "0.24399632162751606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI1ylF4nPT8d"
      },
      "source": [
        "# 課題1\n",
        "* 文書をベクトルとして表現する方法が分かった。\n",
        "* これを使うと、何ができるか？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2caNa_-RQqIT"
      },
      "source": [
        "## 20 newsgroups データセット\n",
        "* 文書分類手法の評価に使う、古典的なデータセット。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6b7-llzQpK9",
        "outputId": "cc8e012b-2e38-4c25-9069-bad4291a1195"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups = fetch_20newsgroups()\n",
        "y_true = newsgroups.target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7ExPmgkROvU"
      },
      "source": [
        "* 下記コードを参考にして、数値を全て「#NUMBER」という特殊な単語へ変換する。\n",
        " * https://scikit-learn.org/stable/auto_examples/bicluster/plot_bicluster_newsgroups.html#sphx-glr-auto-examples-bicluster-plot-bicluster-newsgroups-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFceOyuoQ04u"
      },
      "source": [
        "def number_normalizer(tokens):\n",
        "    \"\"\" Map all numeric tokens to a placeholder.\n",
        "\n",
        "    For many applications, tokens that begin with a number are not directly\n",
        "    useful, but the fact that such a token exists can be relevant.  By applying\n",
        "    this form of dimensionality reduction, some methods may perform better.\n",
        "    \"\"\"\n",
        "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
        "\n",
        "\n",
        "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
        "    def build_tokenizer(self):\n",
        "        tokenize = super().build_tokenizer()\n",
        "        return lambda doc: list(number_normalizer(tokenize(doc)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XasWPbWRWVx"
      },
      "source": [
        "vectorizer = NumberNormalizingVectorizer(stop_words='english', min_df=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsKy592QRYoI"
      },
      "source": [
        "X = vectorizer.fit_transform(newsgroups.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR41GfUJRb4g",
        "outputId": "ba79a6ac-a6a2-43d9-c4b2-bb45a385df96"
      },
      "source": [
        "print(vectorizer.get_feature_names()[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#NUMBER', '_0', '_4', '_5', '_6', '_7u', '_8', '__', '___', '____', '_____', '______', '_______', '________', '_________', '__________', '___________', '____________', '_____________', '______________']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLLwy2wdRi-a",
        "outputId": "6e44b612-3a15-49cf-eef1-11e72fa3c13b"
      },
      "source": [
        "len(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23427"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkp_KASaRqHt"
      },
      "source": [
        "X = X.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9U5NqcMRwW8",
        "outputId": "136ddd19-0ce3-48a7-886b-de722d8d3750"
      },
      "source": [
        "np.dot(X[0], X[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.021480506411432166"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSvAGD5SR0ws",
        "outputId": "3e16de73-3381-499a-ae8b-ff39f97d9359"
      },
      "source": [
        "y_true"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 4, 4, ..., 3, 1, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtn_a7CFR9bC",
        "outputId": "724e9dd9-6a61-4070-b8fc-e2b9437a1ad7"
      },
      "source": [
        "newsgroups.target_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1S-s2gjSAVi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}