{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HaIOpOtFSc4pjrdAglJT6cjgfkrJQIQ4",
      "authorship_tag": "ABX9TyNNZnH8XFm+ZBrLa4ErmEuL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2022-nlp/blob/main/08_document_classification_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtbbXGNJnJQB"
      },
      "source": [
        "# RNNを使った文書分類\n",
        "* RNNの出力を文書の潜在表現として利用し、文書分類を行う。\n",
        " * トークンの埋め込みも含めて学習する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8X-GEo5nqdK"
      },
      "source": [
        "## 全体の準備\n",
        "* ランタイムのタイプをGPUにしておく。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torchdata`をインストールしておく。"
      ],
      "metadata": {
        "id": "8UH3HFGHVSGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "id": "leVZFuhtVPCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 再現性の確保については下記を参照。\n",
        " * https://pytorch.org/docs/stable/notes/randomness.html"
      ],
      "metadata": {
        "id": "dTndo86LVMbn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VicF1RrhJfa"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext import datasets\n",
        "\n",
        "SEED = 123\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6pvVYxeoOqB"
      },
      "source": [
        "## IMDbデータの準備\n",
        "* ここでIMDbデータセットの読み込みにつかう`torchtext.datasets`については、下記を参照。\n",
        " * https://pytorch.org/text/stable/datasets.html\n",
        " * https://torchtext.readthedocs.io/en/latest/datasets.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeCGHBNAojNh"
      },
      "source": [
        "### 訓練データから語彙集合を作成\n",
        "* データの読み込みや前処理は`torchtext`の旧バージョン（0.9より前）とかなり変わっている。\n",
        " * なので、Web検索で`torchtext`について情報収集するときは要注意。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "torchtext.__version__"
      ],
      "metadata": {
        "id": "Wqzi13k9LxZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hm83eK1dzJk"
      },
      "source": [
        "* データセットの訓練データ部分をもとに語彙セットを作成\n",
        " * 語彙セットを作成するとき、テストデータ部分を使わないように。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回の特殊なトークンの設定\n",
        " * 未知語は`<unk>`という特殊なトークンで置き換える。\n",
        " * 同じミニバッチ内で、トークン列の長さが揃っていないとき、末尾は`<pad>`という特殊なトークンで埋める。"
      ],
      "metadata": {
        "id": "bdF4BuD1Vvj2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jug86Tt9hMMD"
      },
      "source": [
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "#tokenizer = get_tokenizer('spacy') # やや時間がかかる。\n",
        "\n",
        "# IMDbデータの訓練データ部分のイテレータを取得\n",
        "train_iter = IMDB(split='train')\n",
        "\n",
        "# 各文書をtokenizeするヘルパ関数\n",
        "def yield_tokens(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "# 最小の出現頻度をmin_freqで指定して語彙サイズを調整する\n",
        "vocab = build_vocab_from_iterator(\n",
        "    yield_tokens(train_iter),\n",
        "    specials=[\"<unk>\", \"<pad>\"],\n",
        "    min_freq=5,\n",
        "    )\n",
        "\n",
        "# OoVトークンには\"<unk>\"のインデックスを返すことにする\n",
        "vocab.set_default_index(vocab[\"<unk>\"]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gechUbtcgFld"
      },
      "source": [
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 以下のように、トークン列をインデックス列へ変換できる。"
      ],
      "metadata": {
        "id": "NFKXZiunV7A3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnnKq-ZDePk3"
      },
      "source": [
        "vocab(['here', 'is', 'an', 'example'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 特殊なトークンには、以下のインデックスが割り振られている。"
      ],
      "metadata": {
        "id": "cKFppgtUWA_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab(['<unk>', '<pad>'])"
      ],
      "metadata": {
        "id": "F6ll_hOzMoA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 単語とインデックスの間の変換は、以下のように行うことができる。"
      ],
      "metadata": {
        "id": "rE9LqkzWYrSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_stoi()['apple']"
      ],
      "metadata": {
        "id": "Y1qPHfkxWgf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_itos()[7316]"
      ],
      "metadata": {
        "id": "RHvtW2ASW4dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDjU2ykppyi2"
      },
      "source": [
        "### IMDBデータを分割してデータセットを作成\n",
        "* 訓練データとテストデータ、二つのイテレータを、データセットへ変換する。\n",
        " * こうしないとsplitできない。\n",
        " * テストデータの方も、こうしないと評価のときエラーが出てしまう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZKysnlAhjGS"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "train_iter, test_iter = IMDB()\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 訓練データ部分から検証データを切り出す"
      ],
      "metadata": {
        "id": "sF636JC2WclW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_train = int(len(train_dataset) * 0.9)\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "print(f\"train size={len(split_train_)} valid size={len(split_valid_)}\")"
      ],
      "metadata": {
        "id": "rrwfmUOMWagv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセットからデータローダを作成"
      ],
      "metadata": {
        "id": "S20UDBOHYiv-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL-3P-txjpK_"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 上のデータローダは、そのままRNNの入力としては使えないミニバッチを与える。\n",
        " * 生のテキストをそのままRNNに与えるわけにはいかない。"
      ],
      "metadata": {
        "id": "Zmdix2OndKqA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOkB3i_Kj0Iw"
      },
      "source": [
        "next(iter(train_dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* RNNの入力に使える形式のミニバッチを作るための関数を定義する。\n",
        " * テキストをtokenizeし、インデックス列へ変換し、paddingして長さを揃える関数。"
      ],
      "metadata": {
        "id": "vYOGyoHIY0Dz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vznl9uJZkpZn"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# paddingに使うトークンのインデックスを取得\n",
        "PAD_IDX = vocab.get_stoi()['<pad>']\n",
        "\n",
        "def collate_batch(batch):\n",
        "  labels = {'neg':0, 'pos':1} #ラベルは0/1に変換\n",
        "  label_list, text_list = [], []\n",
        "  for _label, _text in batch:\n",
        "    label_list.append(labels[_label])\n",
        "    processed_text = torch.tensor(vocab(tokenizer(_text)), dtype=torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "  text_list = pad_sequence(text_list, batch_first=False, padding_value=PAD_IDX)\n",
        "  return label_list.to(device), text_list.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgRH591zmEE4"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_dataloader = DataLoader(\n",
        "    split_train_, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVRHybcmOXr"
      },
      "source": [
        "mini_batch = next(iter(train_dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mini_batch"
      ],
      "metadata": {
        "id": "K9DQfFDWetIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mini_batch[1].shape"
      ],
      "metadata": {
        "id": "HMGkng-1eusc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ90Cx3er_t-"
      },
      "source": [
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57xqsnpTnE_c"
      },
      "source": [
        "## モデルの定義\n",
        "* LSTMを使う（GRUに変えても良い）\n",
        " * http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_3KWMr4hwxl"
      },
      "source": [
        "INPUT_DIM = len(vocab)\n",
        "NUM_CLASS = 2\n",
        "EMBED_DIM = 64\n",
        "HIDDEN_DIM = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1wcVHcmg9KI"
      },
      "source": [
        "class RNNTextSentiment(nn.Module):\n",
        "  def __init__(self, emb_dim, hid_dim,\n",
        "               num_class, vocab_size, padding_idx, p=0.0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.dropout = p\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=padding_idx)\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim)\n",
        "    self.fc = nn.Linear(hid_dim * 2, num_class)\n",
        "    self.dropout = nn.Dropout(p=p)\n",
        "\n",
        "  def forward(self, src):\n",
        "    # srcの形は[単語列長, バッチサイズ]\n",
        "\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "    # embeddedの形は[単語列長, バッチサイズ, 埋め込み次元数]\n",
        "\n",
        "    outputs, (hidden, _) = self.rnn(embedded)\n",
        "    # outputsの形は[単語列長, バッチサイズ, 隠れ状態の次元数]\n",
        "    # hiddenの形は[1, バッチサイズ, 隠れ状態の次元数]\n",
        "\n",
        "    mean_outputs = outputs.mean(0)\n",
        "    hidden = hidden.squeeze()\n",
        "    # mean_outputsの形は[バッチサイズ, 隠れ状態の次元数]\n",
        "    # hiddenの形は[バッチサイズ, 隠れ状態の次元数]\n",
        "    output = self.fc(torch.cat((mean_outputs, hidden), dim=1))\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLo4vO0IrR62"
      },
      "source": [
        "* モデルのインスタンスを得る"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuW6ghef34R4"
      },
      "source": [
        "model = RNNTextSentiment(\n",
        "    EMBED_DIM, HIDDEN_DIM, NUM_CLASS, INPUT_DIM,\n",
        "    padding_idx=PAD_IDX, p=0.5,\n",
        "    ).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S9TDJpIraUM"
      },
      "source": [
        "## 最適化アルゴリズムの設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaEbLC9T4pxb"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t427SeakeqVP"
      },
      "source": [
        "パラメータの数を数えてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h06O037X4vRV"
      },
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwMV61kTri4-"
      },
      "source": [
        "## 損失関数の設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5w-1q7u47Ax"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iScb7iZSs2nY"
      },
      "source": [
        "## 訓練用の関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg1tuw6y4-Or"
      },
      "source": [
        "def train(model, loader, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "\n",
        "  epoch_loss = 0.\n",
        "  epoch_acc = 0.\n",
        "  for label, text in loader:\n",
        "    output = model(text)\n",
        "    loss = criterion(output, label)\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += (output.argmax(1) == label).sum().item()\n",
        "\n",
        "  return epoch_loss / len(loader), epoch_acc / len(loader.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBoo_Ez6s9Gs"
      },
      "source": [
        "## 評価用の関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qmfP-By5fOm"
      },
      "source": [
        "def evaluate(model, loader, criterion):\n",
        "  model.eval()\n",
        "\n",
        "  epoch_loss = 0.\n",
        "  epoch_acc = 0.\n",
        "  with torch.no_grad():\n",
        "    for label, text in loader:\n",
        "      output = model(text)\n",
        "      loss = criterion(output, label)\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += (output.argmax(1) == label).sum().item()\n",
        "\n",
        "  return epoch_loss / len(loader), epoch_acc / len(loader.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 時間表示用の関数"
      ],
      "metadata": {
        "id": "KCvwthmtf5JV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcPnwzJz5rnV"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time // 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1UYirF8NC0g"
      },
      "source": [
        "## 学習の実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioV2XRKG5tf-"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1.\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "  valid_loss, valid_acc = evaluate(model, valid_dataloader, criterion)\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  print(f'Epoch {epoch} | time in {epoch_mins} minutes, {epoch_secs} seconds')\n",
        "  print(f'\\tLoss {train_loss:.4f} (train)\\t|\\tAcc {train_acc * 100:.1f}% (train)')\n",
        "  print(f'\\tLoss {valid_loss:.4f} (valid)\\t|\\tAcc {valid_acc * 100:.1f}% (valid)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6DsW0cQJUMd"
      },
      "source": [
        "# 課題\n",
        "* 上のコードを動かして、感情分析を実践してみよう。\n",
        " * テストデータで評価してみる。\n",
        "* 余裕があれば、ハイパーパラメータをチューニングして、分類性能を上げてみよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBttuNxeJTyd"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kidtt_eGJm--"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS3h2VNjNXNo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzR61Ch2NTU7"
      },
      "source": [
        "## テストデータで評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8ju77-HEhVW"
      },
      "source": [
        "loss, acc = evaluate(model, test_dataloader, criterion)\n",
        "print(f'\\tLoss {loss:.4f} (test)\\t|\\tAcc {acc * 100:.1f}% (test)')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}