{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HaIOpOtFSc4pjrdAglJT6cjgfkrJQIQ4",
      "authorship_tag": "ABX9TyMUtopWREifyBwBMDq8RFUl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2022-nlp/blob/main/08_document_classification_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtbbXGNJnJQB"
      },
      "source": [
        "# RNNを使った文書分類\n",
        "* RNNの出力を文書の潜在表現として利用し、文書分類を行う。\n",
        "* 単語の埋め込みも含めて学習する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8X-GEo5nqdK"
      },
      "source": [
        "## 準備\n",
        "* ランタイムのタイプをGPUにしておこう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 再現性の確保については下記を参照。\n",
        " * https://pytorch.org/docs/stable/notes/randomness.html"
      ],
      "metadata": {
        "id": "dTndo86LVMbn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VicF1RrhJfa"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "rSLmObH0Nbl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6pvVYxeoOqB"
      },
      "source": [
        "## IMDbデータの準備\n",
        "* IMDbデータセットは`torchtext.datasets`から使うこともできる。\n",
        " * https://pytorch.org/text/stable/datasets.html\n",
        " * https://torchtext.readthedocs.io/en/latest/datasets.html\n",
        "* だが、語彙集合を作成するために使う`torchtext.vocab.build_vocab_from_iterator`という関数がとても遅い・・・。\n",
        " * https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        "* なので、ここではCountVectorizerで語彙集合を作ることにした。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml_datasets"
      ],
      "metadata": {
        "id": "vKoXvAg0NY7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ml_datasets import imdb\n",
        "\n",
        "train_data, test_data = imdb()\n",
        "train_texts, train_labels = zip(*train_data)\n",
        "test_texts, test_labels = zip(*test_data)\n",
        "\n",
        "# ラベルは整数の1と0に変換しておく\n",
        "label_id = { \"pos\":1, \"neg\":0 }\n",
        "\n",
        "train_labels = [label_id[label] for label in train_labels]\n",
        "test_labels = [label_id[label] for label in test_labels]"
      ],
      "metadata": {
        "id": "TOpqwvRQNdfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts[0]"
      ],
      "metadata": {
        "id": "jZHbg_gGX3PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[0]"
      ],
      "metadata": {
        "id": "2Il1wPI6X9K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sklearnのCountVectorizerを使ってトークン化\n",
        "* 一般に、語彙集合を確定させるときは、訓練データだけを使う。"
      ],
      "metadata": {
        "id": "ZqZWwl6qNmuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CountVectorizerで語彙集合を作成\n",
        "* ここで作った語彙集合のサイズを、あとで絞る。\n",
        "* `CountVectorizer`の`token_pattern`を指定し、1文字の単語が消えないようにする。\n",
        " * ここでは、テキストを、bag-of-wordsとしてではなく、単語の列としてモデル化する。\n",
        " * その場合、例えば冠詞\"a\"が消えてしまうのは避けたい。\n",
        "* ただし、下の正規表現だと、punctuationは消えてしまう。"
      ],
      "metadata": {
        "id": "pnrxzRfJcraA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# デフォルトの正規表現とは異なる正規表現を使う\n",
        "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "\n",
        "# 訓練データのテキストをトークン化する\n",
        "X = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "# 語彙集合を取得する\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(len(vocabulary))"
      ],
      "metadata": {
        "id": "gZBQI8-gNkSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 単語のdocument frequencyを計算\n",
        "* このままだと語彙サイズが大きすぎる。\n",
        "* 後で、語彙を絞り込むために、document frequencyを使うことにする。\n",
        " * document frequencyが小さい単語は、未知語として扱うことにする。"
      ],
      "metadata": {
        "id": "y33Bp2UMR2pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_freq = np.array((X > 0).sum(0)).squeeze()"
      ],
      "metadata": {
        "id": "vX8YtWJJOqFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* document frequencyでトップ10の単語を見てみる。"
      ],
      "metadata": {
        "id": "aunBPG4Ii_rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabulary[np.argsort(- doc_freq)][:10])"
      ],
      "metadata": {
        "id": "zoLHh0PMi0Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torchtext`の語彙集合の作成\n",
        "* 作り方は下記のリンク先を参照。\n",
        " * https://pytorch.org/text/stable/vocab.html#id1"
      ],
      "metadata": {
        "id": "miVd0kMTN6J6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 単語をキーとするOrderedDictを作成\n",
        " * torchtextで語彙集合を作成するとき、OrderedDictを渡すことになっているため。"
      ],
      "metadata": {
        "id": "LO8hpJeQY6Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "vocab_ordered_dict = OrderedDict(zip(vocabulary, doc_freq))"
      ],
      "metadata": {
        "id": "KOD3IxhoYvlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_ordered_dict)"
      ],
      "metadata": {
        "id": "SUmo6cf8ZB_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `torchtext`の語彙集合を作成\n",
        "* ここでは、document frequencyが10未満の単語を未知語とすることで、語彙サイズを抑えている。\n",
        " * ここはチューニングする余地がある。"
      ],
      "metadata": {
        "id": "bMy7HdAkZtdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import vocab\n",
        "\n",
        "# 未知語は全て\"<unk>\"という特殊なトークンへ置き換えることにする\n",
        "unknown_token = \"<unk>\"\n",
        "\n",
        "# padding用のトークンを作っておく\n",
        "padding_token = \"<pad>\"\n",
        "\n",
        "# OrderedDictをもとにtorchtextでの語彙集合を作成\n",
        "#   min_freqを指定すると、低頻度語は全て未知語として扱われる。\n",
        "#   ここで、OrderedDictの各keyに対応するvalueが用いられる。\n",
        "#   （つまり、document frequency以外の値で未知語を決めても構わない。）\n",
        "vocab = vocab(\n",
        "    vocab_ordered_dict, min_freq=10,\n",
        "    specials=[unknown_token, padding_token],\n",
        "    )\n",
        "\n",
        "# 語彙にない単語のインデックスは全て\"<unk>\"と同じインデックスになるよう、設定する\n",
        "vocab.set_default_index(vocab[unknown_token])\n",
        "\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "um1fTZMSN4NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## テキストをインデックスの列へ変換する関数を定義"
      ],
      "metadata": {
        "id": "mZusQQSTOGKF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnnKq-ZDePk3"
      },
      "source": [
        "# fit済みのCountVectorizerから、前処理とトークナイザを持ってくる\n",
        "preprocessor = vectorizer.build_preprocessor()\n",
        "tokenizer = vectorizer.build_tokenizer()\n",
        "\n",
        "# 前処理、トークナイザ、インデックス列への変換を、一つの処理としてまとめる\n",
        "text_pipeline = lambda x: vocab(tokenizer(preprocessor(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このトークナイザでトークン化すると、punctuationは消えることに注意。"
      ],
      "metadata": {
        "id": "BWNXMqN8f0Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a pen.\"\n",
        "print(preprocessor(text))\n",
        "print(tokenizer(preprocessor(text)))\n",
        "print(text_pipeline(text))"
      ],
      "metadata": {
        "id": "1L8BF5CDOMPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 特殊なトークンには、以下のインデックスが割り振られている。"
      ],
      "metadata": {
        "id": "cKFppgtUWA_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_stoi()['<pad>']"
      ],
      "metadata": {
        "id": "F6ll_hOzMoA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_stoi()['<unk>']"
      ],
      "metadata": {
        "id": "y506iAIqjn-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* インデックスから単語への変換は、以下のように行うことができる。"
      ],
      "metadata": {
        "id": "rE9LqkzWYrSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_itos()[1001]"
      ],
      "metadata": {
        "id": "RHvtW2ASW4dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDjU2ykppyi2"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 全テキストのトークン化\n",
        "* 単語インデックスの列への変換から、テンソルへの変換まで、おこなっている。"
      ],
      "metadata": {
        "id": "CwL1Mcbjf-FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens = [torch.tensor(text_pipeline(text), dtype=torch.int64) for text in train_texts]"
      ],
      "metadata": {
        "id": "NjBz1JNyf9o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tokens[0])"
      ],
      "metadata": {
        "id": "d2kIJIjNghAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokens = [torch.tensor(text_pipeline(text), dtype=torch.int64) for text in test_texts]"
      ],
      "metadata": {
        "id": "MMRg_QmTgpgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 自前のデータセットの定義"
      ],
      "metadata": {
        "id": "VZusvXcvdJUB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZKysnlAhjGS"
      },
      "source": [
        "class MyTextDataset(Dataset):\n",
        "  def __init__(self, labels, tokens):\n",
        "    self.labels = labels\n",
        "    self.tokens = tokens\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.labels[index], self.tokens[index]\n",
        "\n",
        "train_dataset = MyTextDataset(train_labels, train_tokens)\n",
        "test_dataset = MyTextDataset(test_labels, test_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation setを切り分ける"
      ],
      "metadata": {
        "id": "m8Z-2KukdmAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_size = len(train_dataset) // 5\n",
        "train_size = len(train_dataset) - valid_size\n",
        "test_size = len(test_dataset)\n",
        "\n",
        "split_train_, split_valid_ = random_split(train_dataset, [train_size, valid_size])"
      ],
      "metadata": {
        "id": "rrwfmUOMWagv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "S20UDBOHYiv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNNの入力として使えるミニバッチを作る関数を定義\n",
        "* paddingして、同じミニバッチに含まれる単語id列の長さを揃える関数。\n",
        "* この関数は、DataLoaderクラスのインスタンスを作るときに、`collate_fn`の値として指定する。"
      ],
      "metadata": {
        "id": "vYOGyoHIY0Dz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vznl9uJZkpZn"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# paddingに使うトークンのインデックスを取得\n",
        "PAD_IDX = vocab.get_stoi()[padding_token]\n",
        "\n",
        "def collate_batch(batch):\n",
        "  labels, tokens = zip(*batch)\n",
        "  labels = torch.tensor(labels, dtype=torch.int64)\n",
        "  tokens = pad_sequence(tokens, padding_value=PAD_IDX)\n",
        "  return labels, tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 試しにバッチサイズ4でDataLoaderを作って、ミニバッチの中身を見てみる。"
      ],
      "metadata": {
        "id": "-3bHxQQ0p64-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgRH591zmEE4"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, tokens = next(iter(train_loader))\n",
        "print(labels)\n",
        "print(tokens.shape)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "K9DQfFDWetIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ミニバッチのサイズは64にしておく。\n",
        " * ここはチューニングできる。"
      ],
      "metadata": {
        "id": "CLdEWVCtqCpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "HMGkng-1eusc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57xqsnpTnE_c"
      },
      "source": [
        "## モデルの定義\n",
        "* LSTMを使う（GRUに変えても良い）\n",
        " * http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_3KWMr4hwxl"
      },
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "NUM_CLASS = 2\n",
        "EMSIZE = 64\n",
        "HID_DIM = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1wcVHcmg9KI"
      },
      "source": [
        "class RNNTextSentiment(nn.Module):\n",
        "  def __init__(self, emb_dim, hid_dim,\n",
        "               num_class, vocab_size, padding_idx, p=0.0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.dropout = p\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=padding_idx)\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim)\n",
        "    self.fc = nn.Linear(hid_dim * 2, num_class)\n",
        "    self.dropout = nn.Dropout(p=p)\n",
        "\n",
        "  def forward(self, src):\n",
        "    # embeddedの形は(トークン列長, バッチサイズ, 埋め込み次元数)\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "    # outputsの形は(トークン列長, バッチサイズ, 隠れ状態の次元数)\n",
        "    # hiddenの形は(1, バッチサイズ, 隠れ状態の次元数)\n",
        "    outputs, (hidden, _) = self.rnn(embedded)\n",
        "\n",
        "    # mean_outputsの形は(バッチサイズ, 隠れ状態の次元数)\n",
        "    # hiddenの形は(バッチサイズ, 隠れ状態の次元数)\n",
        "    mean_outputs = outputs.mean(0)\n",
        "    hidden = hidden.squeeze()\n",
        "\n",
        "    return self.fc(torch.cat((mean_outputs, hidden), dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuW6ghef34R4"
      },
      "source": [
        "model = RNNTextSentiment(\n",
        "    EMSIZE, HID_DIM, NUM_CLASS, VOCAB_SIZE,\n",
        "    padding_idx=PAD_IDX,\n",
        "    p=0.5,\n",
        "    ).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S9TDJpIraUM"
      },
      "source": [
        "## 最適化アルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaEbLC9T4pxb"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t427SeakeqVP"
      },
      "source": [
        "* パラメータの数を数えてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h06O037X4vRV"
      },
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwMV61kTri4-"
      },
      "source": [
        "## 損失関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5w-1q7u47Ax"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iScb7iZSs2nY"
      },
      "source": [
        "## 訓練を行なう関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg1tuw6y4-Or"
      },
      "source": [
        "def train(dataloader, clip=1.):\n",
        "  model.train()\n",
        "  total_loss, total_acc, total_count = 0, 0, 0\n",
        "  for label, text in dataloader:\n",
        "    label, text = label.to(device), text.to(device)\n",
        "    output = model(text)\n",
        "    loss = criterion(output, label)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    n_instances = label.size(0)\n",
        "    total_loss += loss.item() * n_instances\n",
        "    total_acc += (output.argmax(1) == label).sum().item()\n",
        "    total_count += n_instances\n",
        "  return total_loss / total_count, total_acc / total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBoo_Ez6s9Gs"
      },
      "source": [
        "## 評価を行なう関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qmfP-By5fOm"
      },
      "source": [
        "def evaluate(dataloader):\n",
        "  model.eval()\n",
        "  total_loss, total_acc, total_count = 0, 0, 0\n",
        "  for label, text in dataloader:\n",
        "    label, text = label.to(device), text.to(device)\n",
        "    with torch.no_grad():\n",
        "      output = model(text)\n",
        "      loss = criterion(output, label)\n",
        "      n_instances = label.size(0)\n",
        "      total_loss += loss.item() * n_instances\n",
        "      total_acc += (output.argmax(1) == label).sum().item()\n",
        "      total_count += n_instances\n",
        "  return total_loss / total_count, total_acc / total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 時間表示用の関数"
      ],
      "metadata": {
        "id": "KCvwthmtf5JV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcPnwzJz5rnV"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time // 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1UYirF8NC0g"
      },
      "source": [
        "## 学習の実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioV2XRKG5tf-"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_loader)\n",
        "  valid_loss, valid_acc = evaluate(valid_loader)\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  print(f'Epoch {epoch} | time in {epoch_mins} minutes, {epoch_secs} seconds')\n",
        "  print(f'\\tLoss {train_loss:.4f} (train)\\t|\\tAcc {train_acc*100:.1f}% (train)')\n",
        "  print(f'\\tLoss {valid_loss:.4f} (valid)\\t|\\tAcc {valid_acc*100:.1f}% (valid)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6DsW0cQJUMd"
      },
      "source": [
        "# 課題6\n",
        "* 上のコードを動かして、感情分析を実践してみよう。\n",
        "* 余裕があれば、ハイパーパラメータをチューニングして、分類性能を上げてみよう。\n",
        " * 例えば、LSTMのレイヤ数を2以上にすると、性能は上がるだろうか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBttuNxeJTyd"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kidtt_eGJm--"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS3h2VNjNXNo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzR61Ch2NTU7"
      },
      "source": [
        "## テストデータで評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8ju77-HEhVW"
      },
      "source": [
        "loss, acc = evaluate(test_loader)\n",
        "print(f'\\tLoss {loss:.4f} (test)\\t|\\tAcc {acc*100:.1f}% (test)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRSxKmbEbc7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}