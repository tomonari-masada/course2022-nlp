{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2022-nlp/blob/main/12_interpreting_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQgVszLKQkp"
      },
      "source": [
        "# BERTによるテキスト分類を解釈する"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 参考資料\n",
        " * https://captum.ai/tutorials/IMDB_TorchText_Interpret\n",
        " * https://captum.ai/tutorials/Bert_SQUAD_Interpret"
      ],
      "metadata": {
        "id": "VsdKtVOq3zYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "7H9KW4Cn5ak1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 次のセルは、多分、不要。だから、コメントアウトしてある。\n",
        " * 万が一、何かがインストールされていない、的なエラーが出たら、次のセルを実行してください。"
      ],
      "metadata": {
        "id": "0xWUGkCk5cGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzJ_2vUkl5j0"
      },
      "outputs": [],
      "source": [
        "#!apt install aptitude swig\n",
        "#!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 必要なパッケージをインストール。"
      ],
      "metadata": {
        "id": "XslxKNTb5pNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mecab-python3\n",
        "!pip install fugashi ipadic\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "kH36RXd645dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Captumのインストール"
      ],
      "metadata": {
        "id": "wy9JljQEs7i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Captum （カプタム） は機械学習モデルの解釈のためのライブラリ。"
      ],
      "metadata": {
        "id": "7We78Dq22zia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install captum"
      ],
      "metadata": {
        "id": "gHlpKvPVs7KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## インポート"
      ],
      "metadata": {
        "id": "cb3gdK0u3_84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import AutoModel, AutoTokenizer, BertForSequenceClassification\n",
        "\n",
        "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "\n",
        "MODEL_PATH = \"/content/drive/MyDrive/2022Courses/nlp/bert_for_classification.pt\""
      ],
      "metadata": {
        "id": "iTv9xTsN3KIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ライブドアニュースコーパスの準備"
      ],
      "metadata": {
        "id": "FDHvcDBO7KbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfmhV8kbKPtV"
      },
      "outputs": [],
      "source": [
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 自分のGoogleドライブに移動させておく。"
      ],
      "metadata": {
        "id": "fyRnue196VyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ldcc-20140209.tar.gz /content/drive/MyDrive/data/"
      ],
      "metadata": {
        "id": "6odV0_nePt-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/data/ldcc-20140209.tar.gz\""
      ],
      "metadata": {
        "id": "fpXImnR3aDaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データの読み込みとクレンジング"
      ],
      "metadata": {
        "id": "HdHHMX6itTKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* クレンジング後のデータを、csvファイルとして保存しておく。"
      ],
      "metadata": {
        "id": "wXQoqueW4kGl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfm5JyCClGZh"
      },
      "outputs": [],
      "source": [
        "csv_fname = \"all_text.csv\" \n",
        "\n",
        "def remove_brackets(inp):\n",
        "  brackets_tail = re.compile('【[^】]*】$') #【と】で囲まれた文字列で末尾にあるもの\n",
        "  brackets_head = re.compile('^【[^】]*】') #【と】で囲まれた文字列で先頭にあるもの\n",
        "  return re.sub(brackets_head, '', re.sub(brackets_tail, '', inp))\n",
        "\n",
        "def read_title(f):\n",
        "  next(f) # URLをスキップ\n",
        "  next(f) # タイムスタンプをスキップ\n",
        "  title = next(f) # タイトルを取得\n",
        "  return remove_brackets(title.decode('utf-8'))[:-1]\n",
        "\n",
        "tf = tarfile.open(DATASET_PATH)\n",
        "genre_fnames = {}\n",
        "for ti in tf:\n",
        "  if \"LICENSE.txt\" in ti.name: #ライセンスファイルはスキップ\n",
        "    continue\n",
        "  if len(ti.name.split('/')) < 3: #ディレクトリの深さのチェック\n",
        "    continue\n",
        "  if not ti.name.endswith(\".txt\"): #テキストファイル以外はスキップ\n",
        "    continue\n",
        "  genre = ti.name.split('/')[1] #ジャンルの取得\n",
        "  if not genre in genre_fnames:\n",
        "    genre_fnames[genre] = []\n",
        "  genre_fnames[genre].append(ti.name)\n",
        "\n",
        "with open(csv_fname, \"w\") as wf:\n",
        "  writer = csv.writer(wf)\n",
        "  for i, genre in enumerate(genre_fnames):\n",
        "    for fname in genre_fnames[genre]:\n",
        "      f = tf.extractfile(fname)\n",
        "      title = read_title(f)\n",
        "      row = [genre, i, title]\n",
        "      writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 分類先のクラスの確認"
      ],
      "metadata": {
        "id": "QSIE_v13tXnU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAma_Lfm2Dai"
      },
      "outputs": [],
      "source": [
        "class_names = list(genre_fnames.keys())\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データフレーム化"
      ],
      "metadata": {
        "id": "ALdab05NthC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "101i_nbxlJ-A"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"all_text.csv\", header=None, names=['genre', 'label', 'sentence'])\n",
        "df = df.dropna(how='any') # nanは落とす\n",
        "print(f'num of files： {df.shape[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ここでは、3つのクラスにデータを絞り込むことにする。\n",
        " * 時間節約のため。"
      ],
      "metadata": {
        "id": "AnU6c3Z69RZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['sports-watch', 'movie-enter', 'it-life-hack']\n",
        "df_new = df.query(\"genre in ['sports-watch', 'movie-enter', 'it-life-hack']\")\n",
        "print(f'num of files： {df_new.shape[0]}')\n",
        "display(df_new.sample(10))"
      ],
      "metadata": {
        "id": "t_nhUkem6uAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* labelインデックスの付け直し"
      ],
      "metadata": {
        "id": "LSurr-zL9bXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relabel(class_name):\n",
        "  return class_names.index(class_name)\n",
        "\n",
        "df_new = df.query(\"genre in ['sports-watch', 'movie-enter', 'it-life-hack']\")\n",
        "df_new[\"label\"] = df_new[\"genre\"].apply(relabel)\n",
        "display(df_new.sample(10))"
      ],
      "metadata": {
        "id": "S5mSgU-99ej0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rVcFD8Mleyv"
      },
      "outputs": [],
      "source": [
        "sentences = df_new.sentence.values\n",
        "labels = df_new.label.values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 事前学習済みBERTのトークナイザの準備"
      ],
      "metadata": {
        "id": "RJ5uBnnL7VsG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kcVKzgmljVn"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 試しにトークン化してみる。"
      ],
      "metadata": {
        "id": "R_m-kjOQ4xlD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h87aKaZlmrA"
      },
      "outputs": [],
      "source": [
        "print('text: ', sentences[0])\n",
        "print('tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "print('token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* special tokensを見てみる。"
      ],
      "metadata": {
        "id": "iku9qXIf5FTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7LKdRIte2LS"
      },
      "outputs": [],
      "source": [
        "tokenizer.all_special_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テキストの最大長の調査"
      ],
      "metadata": {
        "id": "dowDkWBB7bWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 最大長でミニバッチの大きさを固定するため。"
      ],
      "metadata": {
        "id": "nZvBy0t9QMPl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk-CbkQ_mPns"
      },
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "longest_sentence = \"\"\n",
        "for sentence in sentences:\n",
        "  token_words = tokenizer.tokenize(sentence)\n",
        "  if len(token_words) > max_len:\n",
        "    max_len = len(token_words)\n",
        "    longest_sentence = sentence\n",
        "print(f\"最大長 = {max_len}\\n{longest_sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット\n",
        "* データセットにアクセスがあるたびにトークナイズする。"
      ],
      "metadata": {
        "id": "zXtDkc9P7frH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iamcYgnd_17s"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_len):\n",
        "    super(MyDataset, self).__init__()\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def encode(self, text):\n",
        "    return self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        padding='max_length',\n",
        "        )\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    text = self.texts[index]\n",
        "    encoded = self.encode(text)\n",
        "    return (\n",
        "        torch.tensor(encoded['input_ids']).long(),\n",
        "        torch.tensor(encoded['attention_mask']).long(),\n",
        "        self.labels[index],\n",
        "        text,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 最大長に2を足しているのは`[CLS]`と`[SEP]`の分（下のセルを参照）"
      ],
      "metadata": {
        "id": "PtzGQ_4y7lyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toJvWP6vBAC0"
      },
      "outputs": [],
      "source": [
        "dataset = MyDataset(sentences, labels, tokenizer, max_len+2)\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセットの分割\n",
        "* 訓練：検証：テスト = 8:1:1とした。"
      ],
      "metadata": {
        "id": "Vjf1aSMm7rgE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwsEX5OsndpR"
      },
      "outputs": [],
      "source": [
        "valid_size = len(dataset) // 10\n",
        "test_size = valid_size\n",
        "train_size = len(dataset) - valid_size - test_size\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "print(f\"訓練データ数={train_size} 検証データ数={valid_size} テストデータ数={test_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データローダ"
      ],
      "metadata": {
        "id": "LT__i-qr74Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "y-d9szWD71Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 事前学習済みBERTの準備\n",
        "* これをfinetuneする。"
      ],
      "metadata": {
        "id": "uy1RMH7z8DKS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZB99TQsoKb9"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(class_names),\n",
        "    output_attentions=True,\n",
        "    output_hidden_states=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回は、BERT本体のrequires_gradはFalseにし、分類用の全結合層だけをtrainingすることにする。\n",
        " * 単に説明のための時間を短縮したいからで、こうしないほうが分類性能は良くなる。\n",
        " * 参考情報: 全結合層だけtrainingしてから、全体のfinetuningをすると良いという話もある。 ( https://arxiv.org/abs/2202.10054 )"
      ],
      "metadata": {
        "id": "gwdXyx0W5646"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G7rMlOVq4yk"
      },
      "outputs": [],
      "source": [
        "for param in model.base_model.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finetuningの実行"
      ],
      "metadata": {
        "id": "Ugopm-Bv9Ezr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* GPUの設定"
      ],
      "metadata": {
        "id": "Na8aLzjs8IAh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBC-P0rylffg"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* すでにfinetuneしたモデルがあるときは、以下のようにpathを指定して読み込む。"
      ],
      "metadata": {
        "id": "Y72B6psv5vUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ],
      "metadata": {
        "id": "9TIKfmfu5qi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* すでにfinetuneしたモデルを使う場合は、以下のfinetuningのコードは動かさなくてよい。"
      ],
      "metadata": {
        "id": "FtsMf2Qd519Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* オプティマイザの準備"
      ],
      "metadata": {
        "id": "_n9QI2XA9CRZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhwFYqT5nZyC"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TbjSD2Og24Q"
      },
      "outputs": [],
      "source": [
        "NUM_TRAIN_EPOCHS = 20\n",
        "\n",
        "for epoch in range(1, NUM_TRAIN_EPOCHS+1):\n",
        "\n",
        "  model.train()\n",
        "  train_losses = []\n",
        "  for batch in train_dataloader:\n",
        "    ids = batch[0].to(device)\n",
        "    mask = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "    loss = model(ids, mask, labels=labels).loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    train_losses.append(loss.item())\n",
        "  print(f\"epoch {epoch} | train loss {sum(train_losses)/len(train_losses):.4f}\", end=\" \")\n",
        "\n",
        "  model.eval()\n",
        "  valid_losses = []\n",
        "  for batch in valid_dataloader:\n",
        "    ids = batch[0].to(device)\n",
        "    mask = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "    with torch.no_grad():\n",
        "      loss = model(ids, mask, labels=labels).loss\n",
        "    valid_losses.append(loss.item())\n",
        "  print(f\"| valid loss {sum(valid_losses)/len(valid_losses):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルの保存"
      ],
      "metadata": {
        "id": "D9VWAGWDxa35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), MODEL_PATH)"
      ],
      "metadata": {
        "id": "V8x4pgPWxaad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finetuningしたモデルの評価"
      ],
      "metadata": {
        "id": "LWzpB8bs9ThW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 正解率を求めるヘルパ関数の定義"
      ],
      "metadata": {
        "id": "z2SrWdx79N2O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYClkhq6x04B"
      },
      "outputs": [],
      "source": [
        "def evaluation(dataloader):\n",
        "  model.eval()\n",
        "  n_correct_answers = 0\n",
        "  n_instances = 0\n",
        "  for batch in dataloader:\n",
        "    ids = batch[0].to(device)\n",
        "    mask = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = model(ids, mask).logits\n",
        "    predicted_class_id = logits.argmax(-1)\n",
        "    n_correct_answers += (predicted_class_id == labels).sum()\n",
        "    n_instances += len(labels)\n",
        "  return n_correct_answers, n_instances"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_correct_answers, n_instances = evaluation(valid_dataloader)\n",
        "print(f\"classification accuracy={n_correct_answers / n_instances:.3f}\")"
      ],
      "metadata": {
        "id": "yHcfuo1fqf-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Captumを使う"
      ],
      "metadata": {
        "id": "EfsbTO8G6lrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### パディング用トークンのインデックスを取得\n",
        " * 何の情報も持たない(＝baselineとなる)トークン列を作るために、必要となる。"
      ],
      "metadata": {
        "id": "lWczJHEe0pxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IND = tokenizer.pad_token_id\n",
        "print(PAD_IND)"
      ],
      "metadata": {
        "id": "9J4GUC6q0klQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* パディング用トークンをリファレンストークンとして設定する。"
      ],
      "metadata": {
        "id": "AQrfL_ygR2VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)"
      ],
      "metadata": {
        "id": "tJ0NiXt_0wGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 指定されたラベルの確率を返す関数を定義しておく。\n",
        " * 後で必要になるため。"
      ],
      "metadata": {
        "id": "AVfrYD8M6GYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_forward_func(ids, mask, label):\n",
        "  pred_probs = torch.softmax(model(ids, mask).logits, -1)\n",
        "  return pred_probs[:,label]"
      ],
      "metadata": {
        "id": "jERZi9mc6Fuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 定義した関数の動作を確認する。"
      ],
      "metadata": {
        "id": "MUiLA7An6Q8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids, mask, true_class, text = test_dataset[0]\n",
        "ids = ids.unsqueeze(0).to(device)\n",
        "mask = mask.unsqueeze(0).to(device)\n",
        "print(bert_forward_func(ids, mask, true_class))"
      ],
      "metadata": {
        "id": "F8IfNmYa6P3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 解釈手法（Integrated Gradients）の準備\n",
        "* それに対するパラメータの寄与を解釈したい値を返す関数を、指定する。\n",
        "* また、attributionを算出したいパラメータを、指定する。"
      ],
      "metadata": {
        "id": "oLkaoNWM5Kv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lig = LayerIntegratedGradients(bert_forward_func, model.bert.embeddings.word_embeddings)"
      ],
      "metadata": {
        "id": "sIqF-Cv61ghN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 可視化のヘルパ関数"
      ],
      "metadata": {
        "id": "sY_q8gky7TBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://github.com/pytorch/captum/blob/master/captum/attr/_utils/visualization.py#L755"
      ],
      "metadata": {
        "id": "6uaK-lkv7yEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_attributions_to_visualizer(attributions, text, pred_prob, pred_class, true_class,\n",
        "                                   attr_class, convergence_scores, vis_data_records):\n",
        "\n",
        "  attributions = attributions.sum(dim=2).squeeze(0)\n",
        "  attributions = attributions / torch.norm(attributions)\n",
        "  attributions = attributions.cpu().detach().numpy()\n",
        "\n",
        "  # storing couple samples in an array for visualization purposes\n",
        "  vis_data_records.append(visualization.VisualizationDataRecord(\n",
        "      attributions,\n",
        "      pred_prob,\n",
        "      pred_class,\n",
        "      true_class,\n",
        "      attr_class,\n",
        "      attributions.sum(),\n",
        "      text,\n",
        "      convergence_scores))"
      ],
      "metadata": {
        "id": "Aqd1eUP77RZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テキストの分類結果を解釈するためのヘルパ関数"
      ],
      "metadata": {
        "id": "mqqBVFe19Q_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret_sentence(sentence, label):\n",
        "  encoded = dataset.encode(sentence)\n",
        "\n",
        "  indexed = encoded.input_ids\n",
        "  seq_length = len(indexed)\n",
        "  input_indices = torch.tensor(indexed).long()\n",
        "  input_indices = input_indices.unsqueeze(0).to(device)\n",
        "\n",
        "  mask = torch.tensor(encoded.attention_mask).long()\n",
        "  mask = mask.unsqueeze(0).to(device)\n",
        "\n",
        "  text = dataset.tokenizer.convert_ids_to_tokens(indexed)\n",
        "\n",
        "  # generate reference indices for each sample\n",
        "  reference_indices = token_reference.generate_reference(seq_length, device=device)\n",
        "  reference_indices = reference_indices.unsqueeze(0).to(device)\n",
        "  \n",
        "  pred_probs = torch.softmax(model(input_indices, mask).logits, -1).squeeze()\n",
        "  pred_ind = pred_probs.argmax().item()\n",
        "\n",
        "  # compute attributions and approximation delta using layer integrated gradients\n",
        "  attributions_ig, delta = lig.attribute(\n",
        "      input_indices,\n",
        "      reference_indices,\n",
        "      additional_forward_args=(mask, pred_ind),\n",
        "      return_convergence_delta=True,\n",
        "      )\n",
        "\n",
        "  add_attributions_to_visualizer(\n",
        "    attributions_ig, \n",
        "    text, \n",
        "    pred_probs[pred_ind], \n",
        "    class_names[pred_ind], \n",
        "    class_names[label],\n",
        "    class_names[pred_ind],\n",
        "    delta, \n",
        "    vis_data_records_ig)"
      ],
      "metadata": {
        "id": "TC_H-X_dvK5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "LxQv-uopV8Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis_data_records_ig = []"
      ],
      "metadata": {
        "id": "iXORZgMZ7EjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids, mask, true_class, text = test_dataset[0]\n",
        "interpret_sentence(text, label=true_class)"
      ],
      "metadata": {
        "id": "WoEWcqUwVADm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = visualization.visualize_text(vis_data_records_ig)"
      ],
      "metadata": {
        "id": "bHzPqo1pWDer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWq2_Kl-Zz5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "12UP3SmTnC1V2keRdSXwiwOzTlm4_8Od9",
      "authorship_tag": "ABX9TyPMrJPw+1Yv45sBu7kd4dQC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}