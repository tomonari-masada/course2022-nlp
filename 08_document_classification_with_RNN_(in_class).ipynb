{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HaIOpOtFSc4pjrdAglJT6cjgfkrJQIQ4",
      "authorship_tag": "ABX9TyMsIUN99xn6VUGIMKVzrVj3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2022-nlp/blob/main/08_document_classification_with_RNN_(in_class).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtbbXGNJnJQB"
      },
      "source": [
        "# RNNを使った文書分類\n",
        "* RNNの出力を文書の潜在表現として利用し、文書分類を行う。\n",
        "* 単語の埋め込みも含めて学習する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8X-GEo5nqdK"
      },
      "source": [
        "## 準備\n",
        "* ランタイムのタイプをGPUにしておこう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 再現性の確保については下記を参照。\n",
        " * https://pytorch.org/docs/stable/notes/randomness.html"
      ],
      "metadata": {
        "id": "dTndo86LVMbn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VicF1RrhJfa"
      },
      "source": [
        "import time\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "#from torchtext.vocab import vocab # ここに書いておくとエラーが出る？？？\n",
        "\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "rSLmObH0Nbl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57635214-3333-4b31-f662-4bf67f191d10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6pvVYxeoOqB"
      },
      "source": [
        "## IMDbデータの準備\n",
        "* IMDbデータセットは`torchtext.datasets`から使うこともできる。\n",
        " * https://pytorch.org/text/stable/datasets.html\n",
        " * https://torchtext.readthedocs.io/en/latest/datasets.html\n",
        "* だが、語彙集合を作成するために使う`torchtext.vocab.build_vocab_from_iterator`という関数がとても遅い・・・。\n",
        " * https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        "* なので、ここではCountVectorizerで語彙集合を作ることにした。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml_datasets"
      ],
      "metadata": {
        "id": "vKoXvAg0NY7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89919db-8420-4f7c-9a99-c975ea10e01a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ml_datasets\n",
            "  Downloading ml_datasets-0.2.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from ml_datasets) (4.64.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from ml_datasets) (2.4.5)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from ml_datasets) (1.21.6)\n",
            "Requirement already satisfied: catalogue<3.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ml_datasets) (2.0.8)\n",
            "Installing collected packages: ml-datasets\n",
            "Successfully installed ml-datasets-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ml_datasets import imdb\n",
        "\n",
        "train_data, test_data = imdb()\n",
        "train_texts, train_labels = zip(*train_data)\n",
        "test_texts, test_labels = zip(*test_data)\n",
        "\n",
        "# ラベルは整数の1と0に変換しておく\n",
        "label_id = { \"pos\":1, \"neg\":0 }\n",
        "\n",
        "train_labels = [label_id[label] for label in train_labels]\n",
        "test_labels = [label_id[label] for label in test_labels]"
      ],
      "metadata": {
        "id": "TOpqwvRQNdfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc189ca7-ba30-429e-c6dd-a41fcbe8712c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "84131840it [00:08, 9410037.95it/s]                               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Untaring file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts[0]"
      ],
      "metadata": {
        "id": "jZHbg_gGX3PV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "454c7a2d-7f3a-4f2c-fc27-553c4490803f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tight script, good direction, excellent performances, strong cast, effective use of locations....\\n\\n\\n\\nPaul McGann gives a detailed, subtle performance as the man in the centre of a new murder investigation who may just have committed a similar murder previously.\\n\\n\\n\\nThere is an interesting moral & emotional journey happening with his character (Ben Turner) and it intersects with the journey undertaken by Amanda Burton. Inevitably they cross over... Who has done what?\\n\\n\\n\\nThe examination of WHY, both in the past and in the present, rather than WHO might have yielded a more interesting, Dostoyevskian story, but hey, who's complaining?\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[0]"
      ],
      "metadata": {
        "id": "2Il1wPI6X9K3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ca5eac-3419-475c-9b32-9743ee622196"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sklearnのCountVectorizerによるトークン化\n",
        "* 一般に、語彙集合を確定させるときは、訓練データだけを使う。"
      ],
      "metadata": {
        "id": "ZqZWwl6qNmuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CountVectorizerで語彙集合を作成\n",
        "* ここで作った語彙集合のサイズを、あとで絞る。\n",
        "* `CountVectorizer`の`token_pattern`を指定し、1文字の単語が消えないようにする。\n",
        " * ここでは、テキストを、bag-of-wordsとしてではなく、単語の列としてモデル化する。\n",
        " * その場合、例えば冠詞\"a\"が消えてしまうのは避けたい。\n",
        "* ただし、下の正規表現だと、punctuationは消えてしまう。"
      ],
      "metadata": {
        "id": "pnrxzRfJcraA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デフォルトの正規表現とは異なる正規表現を使う\n",
        "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "\n",
        "# 訓練データのテキストをトークン化する\n",
        "X = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "# 語彙集合を取得する\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(len(vocabulary))"
      ],
      "metadata": {
        "id": "gZBQI8-gNkSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce28ff9c-305f-4fa5-e0a2-5cb743dc64d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 単語のdocument frequencyを計算\n",
        "* このままだと語彙サイズが大きすぎる。\n",
        "* 後で、語彙を絞り込むために、document frequencyを使うことにする。\n",
        " * document frequencyが小さい単語は、未知語として扱うことにする。"
      ],
      "metadata": {
        "id": "y33Bp2UMR2pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_freq = np.array((X > 0).sum(0)).squeeze()"
      ],
      "metadata": {
        "id": "vX8YtWJJOqFa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* document frequencyでトップ10の単語を見てみる。"
      ],
      "metadata": {
        "id": "aunBPG4Ii_rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabulary[np.argsort(- doc_freq)][:10])"
      ],
      "metadata": {
        "id": "zoLHh0PMi0Nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb441c91-2a01-409e-b5ae-7720ad4ca873"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the' 'a' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torchtext`の語彙集合の作成\n",
        "* 作り方は下記のリンク先を参照。\n",
        " * https://pytorch.org/text/stable/vocab.html#id1"
      ],
      "metadata": {
        "id": "miVd0kMTN6J6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OrderedDictを作成\n",
        " * torchtextで語彙集合を作成するとき、OrderedDictを渡すことになっているため。"
      ],
      "metadata": {
        "id": "LO8hpJeQY6Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_ordered_dict = OrderedDict(zip(vocabulary, doc_freq))\n",
        "len(vocab_ordered_dict)"
      ],
      "metadata": {
        "id": "KOD3IxhoYvlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda4550b-da47-4b34-ac25-67c8cffd6ab8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74891"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 適当な単語のdocument frequencyを見てみる。"
      ],
      "metadata": {
        "id": "1xE_CWrZHs5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_ordered_dict[\"apple\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL3QgJxdzwNb",
        "outputId": "e7298fef-0f64-48dc-b2de-9187ddf9636d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `torchtext`の語彙集合を作成\n",
        "* ここでは、document frequencyが10未満の単語を未知語とすることで、語彙サイズを抑えている。\n",
        " * ここはチューニングする余地がある。"
      ],
      "metadata": {
        "id": "bMy7HdAkZtdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import vocab\n",
        "\n",
        "# 未知語は全て\"<unk>\"という特殊なトークンへ置き換えることにする\n",
        "unknown_token = \"<unk>\"\n",
        "\n",
        "# padding用のトークンを作っておく\n",
        "padding_token = \"<pad>\"\n",
        "\n",
        "# OrderedDictをもとにtorchtextでの語彙集合を作成\n",
        "#   min_freqを指定すると、低頻度語は全て未知語として扱われる。\n",
        "#   ここで、OrderedDictの各keyに対応するvalueが用いられる。\n",
        "#   （つまり、document frequency以外の値で未知語を決めても構わない。）\n",
        "vocab = vocab(\n",
        "    vocab_ordered_dict, min_freq=10,\n",
        "    specials=[unknown_token, padding_token],\n",
        "    )\n",
        "\n",
        "# 語彙にない単語のインデックスは全て\"<unk>\"と同じインデックスになるよう、設定する\n",
        "vocab.set_default_index(vocab[unknown_token])\n",
        "\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "um1fTZMSN4NI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9cef1a-e8f8-44a6-dfb7-58fac31a88db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## テキストを単語idの列へ変換する関数"
      ],
      "metadata": {
        "id": "mZusQQSTOGKF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnnKq-ZDePk3"
      },
      "source": [
        "# fit済みのCountVectorizerから、前処理とトークナイザを持ってくる\n",
        "preprocessor = vectorizer.build_preprocessor()\n",
        "tokenizer = vectorizer.build_tokenizer()\n",
        "\n",
        "# 前処理、トークナイザ、インデックス列への変換を、一つの処理としてまとめる\n",
        "text_pipeline = lambda x: vocab(tokenizer(preprocessor(x)))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このトークナイザでトークン化すると、punctuationは消えることに注意。"
      ],
      "metadata": {
        "id": "BWNXMqN8f0Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a fnekjfjd.\"\n",
        "print(preprocessor(text))\n",
        "print(tokenizer(preprocessor(text)))\n",
        "print(text_pipeline(text))"
      ],
      "metadata": {
        "id": "1L8BF5CDOMPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e605e2-5881-4d2f-b2dc-c9c66081ce6a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is a fnekjfjd.\n",
            "['this', 'is', 'a', 'fnekjfjd']\n",
            "[16665, 8906, 272, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 特殊なトークンには、以下のインデックスが割り振られている。"
      ],
      "metadata": {
        "id": "cKFppgtUWA_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_stoi()['<pad>']"
      ],
      "metadata": {
        "id": "F6ll_hOzMoA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56196cac-75f7-42df-f516-7857e20e395b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_stoi()['<unk>']"
      ],
      "metadata": {
        "id": "y506iAIqjn-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04e74c9-d299-4994-a881-303e31fb3258"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* インデックスから単語への変換は、以下のように行うことができる。"
      ],
      "metadata": {
        "id": "rE9LqkzWYrSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.get_itos()[1001]"
      ],
      "metadata": {
        "id": "RHvtW2ASW4dr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "01660f2d-caa1-4907-d5d4-271bc573fa16"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'apple'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDjU2ykppyi2"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 全テキストのトークン化\n",
        "* 単語インデックスの列への変換から、テンソルへの変換まで、おこなっている。"
      ],
      "metadata": {
        "id": "CwL1Mcbjf-FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens = [torch.tensor(text_pipeline(text), dtype=torch.int64) for text in train_texts]\n",
        "test_tokens = [torch.tensor(text_pipeline(text), dtype=torch.int64) for text in test_texts]"
      ],
      "metadata": {
        "id": "NjBz1JNyf9o2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tokens[0])"
      ],
      "metadata": {
        "id": "d2kIJIjNghAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca19f987-f42e-4068-92ba-e34b6439041f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([16755, 14467,  7232,  4810,  5922, 12107, 15926,  2682,  5434, 17587,\n",
            "        11473,  9825, 12012,     0,  7139,   272,  4651, 16030, 12106,  1149,\n",
            "        16598, 10126,  8430, 16598,  2785, 11473,   272, 11190, 10952,  8851,\n",
            "        18178, 10332,  9166,  7711,  3403,   272, 14973, 10952, 12736, 16629,\n",
            "         8906,   819,  8755, 10816,  5574,  9111,  7623, 18292,  7959,  2852,\n",
            "         1724, 17184,   843,  8930,     0, 18292, 16598,  9111,     0,  2446,\n",
            "          760,  2407,  8547, 16643,  4053, 11720, 18178,  7681,  5062, 18130,\n",
            "        16598,  5909, 11473, 18196,  2102,  8430, 16598, 11968,   843,  8430,\n",
            "        16598, 12684, 13259, 16587, 18178, 10558,  7711,     0,   272, 10826,\n",
            "         8755,     0, 15845,  2429,  7893, 18178, 14194,  3458])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 自前のデータセットの定義"
      ],
      "metadata": {
        "id": "VZusvXcvdJUB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZKysnlAhjGS"
      },
      "source": [
        "class MyTextDataset(Dataset):\n",
        "  def __init__(self, labels, tokens):\n",
        "    self.labels = labels\n",
        "    self.tokens = tokens\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.labels[index], self.tokens[index]\n",
        "\n",
        "train_dataset = MyTextDataset(train_labels, train_tokens)\n",
        "test_dataset = MyTextDataset(test_labels, test_tokens)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation setの切り分け"
      ],
      "metadata": {
        "id": "m8Z-2KukdmAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_size = len(train_dataset) // 5\n",
        "train_size = len(train_dataset) - valid_size\n",
        "test_size = len(test_dataset)\n",
        "\n",
        "split_train_, split_valid_ = random_split(train_dataset, [train_size, valid_size])"
      ],
      "metadata": {
        "id": "rrwfmUOMWagv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "S20UDBOHYiv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNNの入力として使えるミニバッチを作る関数を定義\n",
        "* paddingして、同じミニバッチに含まれる単語id列の長さを揃える関数。\n",
        "* この関数は、DataLoaderクラスのインスタンスを作るときに、`collate_fn`の値として指定する。"
      ],
      "metadata": {
        "id": "vYOGyoHIY0Dz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vznl9uJZkpZn"
      },
      "source": [
        "# paddingに使うトークンのインデックスを取得\n",
        "PAD_IDX = vocab.get_stoi()[padding_token]\n",
        "\n",
        "def collate_batch(batch):\n",
        "  labels, tokens = zip(*batch)\n",
        "  labels = torch.tensor(labels, dtype=torch.int64)\n",
        "  tokens = pad_sequence(tokens, padding_value=PAD_IDX)\n",
        "  return labels, tokens"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 試しにバッチサイズ4でDataLoaderを作って、ミニバッチの中身を見てみる。"
      ],
      "metadata": {
        "id": "-3bHxQQ0p64-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgRH591zmEE4"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, tokens = next(iter(train_loader))\n",
        "print(labels)\n",
        "print(tokens.shape)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "K9DQfFDWetIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b2b25c-fe34-44ba-cc84-9264cb4e66b5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 1, 1])\n",
            "torch.Size([729, 4])\n",
            "tensor([[16598, 16665,  2685, 16665],\n",
            "        [    0,  8554, 11473,  8906],\n",
            "        [ 8906,  5630,  1957, 11543],\n",
            "        ...,\n",
            "        [    1, 16806,     1,     1],\n",
            "        [    1,  7959,     1,     1],\n",
            "        [    1, 14600,     1,     1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ミニバッチのサイズは64にしておく。\n",
        " * ここはチューニングできる。"
      ],
      "metadata": {
        "id": "CLdEWVCtqCpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "HMGkng-1eusc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57xqsnpTnE_c"
      },
      "source": [
        "## モデル\n",
        "* LSTMを使う（GRUに変えても良い）\n",
        " * http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_3KWMr4hwxl"
      },
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "NUM_CLASS = 2\n",
        "EMSIZE = 64\n",
        "HID_DIM = 64"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1wcVHcmg9KI"
      },
      "source": [
        "class RNNTextSentiment(nn.Module):\n",
        "  def __init__(self, emb_dim, hid_dim,\n",
        "               num_class, vocab_size, padding_idx, p=0.0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.dropout = p\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=padding_idx)\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim)\n",
        "    self.fc = nn.Linear(hid_dim * 2, num_class)\n",
        "    self.dropout = nn.Dropout(p=p)\n",
        "\n",
        "  def forward(self, src):\n",
        "    # embeddedの形は(トークン列長, バッチサイズ, 埋め込み次元数)\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "    # outputsの形は(トークン列長, バッチサイズ, 隠れ状態の次元数)\n",
        "    # hiddenの形は(1, バッチサイズ, 隠れ状態の次元数)\n",
        "    outputs, (hidden, _) = self.rnn(embedded)\n",
        "\n",
        "    # mean_outputsの形は(バッチサイズ, 隠れ状態の次元数)\n",
        "    # hiddenの形は(バッチサイズ, 隠れ状態の次元数)\n",
        "    mean_outputs = outputs.mean(0)\n",
        "    hidden = hidden.squeeze()\n",
        "\n",
        "    return self.fc(torch.cat((mean_outputs, hidden), dim=1))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuW6ghef34R4"
      },
      "source": [
        "model = RNNTextSentiment(\n",
        "    EMSIZE, HID_DIM, NUM_CLASS, VOCAB_SIZE,\n",
        "    padding_idx=PAD_IDX,\n",
        "    p=0.5,\n",
        "    ).to(device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S9TDJpIraUM"
      },
      "source": [
        "## 最適化アルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaEbLC9T4pxb"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t427SeakeqVP"
      },
      "source": [
        "* パラメータの数を数えてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h06O037X4vRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ea53ac-8128-4995-c499-9435c408214f"
      },
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters.')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,221,570 trainable parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* token embeddingに使うパラメータの個数だけを数えてみる。"
      ],
      "metadata": {
        "id": "1Dhlsm6qIrCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_embedding_parameters(model):\n",
        "  return sum(p.numel() for p in model.embedding.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_embedding_parameters(model):,} trainable embedding parameters.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za_uJlEdIid7",
        "outputId": "de00ee42-1bcd-4816-ef32-f79ced194064"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,188,032 trainable embedding parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwMV61kTri4-"
      },
      "source": [
        "## 損失関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5w-1q7u47Ax"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iScb7iZSs2nY"
      },
      "source": [
        "## 訓練を行なう関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg1tuw6y4-Or"
      },
      "source": [
        "def train(dataloader, clip=1.):\n",
        "  model.train()\n",
        "  total_loss, total_acc, total_count = 0, 0, 0\n",
        "  for label, text in dataloader:\n",
        "    label, text = label.to(device), text.to(device)\n",
        "    output = model(text)\n",
        "    loss = criterion(output, label)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    n_instances = label.size(0)\n",
        "    total_loss += loss.item() * n_instances\n",
        "    total_acc += (output.argmax(1) == label).sum().item()\n",
        "    total_count += n_instances\n",
        "  return total_loss / total_count, total_acc / total_count"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBoo_Ez6s9Gs"
      },
      "source": [
        "## 評価を行なう関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qmfP-By5fOm"
      },
      "source": [
        "def evaluate(dataloader):\n",
        "  model.eval()\n",
        "  total_loss, total_acc, total_count = 0, 0, 0\n",
        "  for label, text in dataloader:\n",
        "    label, text = label.to(device), text.to(device)\n",
        "    with torch.no_grad():\n",
        "      output = model(text)\n",
        "      loss = criterion(output, label)\n",
        "      n_instances = label.size(0)\n",
        "      total_loss += loss.item() * n_instances\n",
        "      total_acc += (output.argmax(1) == label).sum().item()\n",
        "      total_count += n_instances\n",
        "  return total_loss / total_count, total_acc / total_count"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 時間表示用の関数"
      ],
      "metadata": {
        "id": "KCvwthmtf5JV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcPnwzJz5rnV"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time // 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1UYirF8NC0g"
      },
      "source": [
        "## 学習の実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioV2XRKG5tf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910745fc-4681-4caf-e6d6-6614eb5c20e7"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_loader)\n",
        "  valid_loss, valid_acc = evaluate(valid_loader)\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  print(f'Epoch {epoch} | time in {epoch_mins} minutes, {epoch_secs} seconds')\n",
        "  print(f'\\tLoss {train_loss:.4f} (train)\\t|\\tAcc {train_acc*100:.1f}% (train)')\n",
        "  print(f'\\tLoss {valid_loss:.4f} (valid)\\t|\\tAcc {valid_acc*100:.1f}% (valid)')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | time in 0 minutes, 4 seconds\n",
            "\tLoss 0.6749 (train)\t|\tAcc 57.1% (train)\n",
            "\tLoss 0.6183 (valid)\t|\tAcc 68.1% (valid)\n",
            "Epoch 2 | time in 0 minutes, 3 seconds\n",
            "\tLoss 0.5938 (train)\t|\tAcc 69.4% (train)\n",
            "\tLoss 0.5248 (valid)\t|\tAcc 76.1% (valid)\n",
            "Epoch 3 | time in 0 minutes, 2 seconds\n",
            "\tLoss 0.5281 (train)\t|\tAcc 75.5% (train)\n",
            "\tLoss 0.6024 (valid)\t|\tAcc 71.0% (valid)\n",
            "Epoch 4 | time in 0 minutes, 2 seconds\n",
            "\tLoss 0.4799 (train)\t|\tAcc 79.4% (train)\n",
            "\tLoss 0.5898 (valid)\t|\tAcc 76.9% (valid)\n",
            "Epoch 5 | time in 0 minutes, 2 seconds\n",
            "\tLoss 0.4330 (train)\t|\tAcc 81.2% (train)\n",
            "\tLoss 0.4174 (valid)\t|\tAcc 83.2% (valid)\n",
            "Epoch 6 | time in 0 minutes, 2 seconds\n",
            "\tLoss 0.4063 (train)\t|\tAcc 82.7% (train)\n",
            "\tLoss 0.4175 (valid)\t|\tAcc 83.4% (valid)\n",
            "Epoch 7 | time in 0 minutes, 3 seconds\n",
            "\tLoss 0.3643 (train)\t|\tAcc 84.7% (train)\n",
            "\tLoss 0.4024 (valid)\t|\tAcc 84.6% (valid)\n",
            "Epoch 8 | time in 0 minutes, 3 seconds\n",
            "\tLoss 0.3416 (train)\t|\tAcc 85.8% (train)\n",
            "\tLoss 0.4610 (valid)\t|\tAcc 84.2% (valid)\n",
            "Epoch 9 | time in 0 minutes, 3 seconds\n",
            "\tLoss 0.3156 (train)\t|\tAcc 87.3% (train)\n",
            "\tLoss 0.4188 (valid)\t|\tAcc 86.2% (valid)\n",
            "Epoch 10 | time in 0 minutes, 3 seconds\n",
            "\tLoss 0.2911 (train)\t|\tAcc 88.0% (train)\n",
            "\tLoss 0.3611 (valid)\t|\tAcc 87.3% (valid)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6DsW0cQJUMd"
      },
      "source": [
        "# 課題6\n",
        "* 上のコードを動かして、感情分析を実践してみよう。\n",
        "* 余裕があれば、ハイパーパラメータをチューニングして、分類性能を上げてみよう。\n",
        " * 例えば、LSTMのレイヤ数を2以上にすると、性能は上がるだろうか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBttuNxeJTyd"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kidtt_eGJm--"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS3h2VNjNXNo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzR61Ch2NTU7"
      },
      "source": [
        "## テストデータで評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8ju77-HEhVW"
      },
      "source": [
        "loss, acc = evaluate(test_loader)\n",
        "print(f'\\tLoss {loss:.4f} (test)\\t|\\tAcc {acc*100:.1f}% (test)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRSxKmbEbc7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}