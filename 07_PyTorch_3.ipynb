{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Ju5Lc6g45t9qjI0R2cFUAA1wHBFm2IeB",
      "authorship_tag": "ABX9TyOjLxyujSDUcofLs53gnsSj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2022-nlp/blob/main/07_PyTorch_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hx2zN0IvyqF"
      },
      "source": [
        "# PyTorch入門 (3)\n",
        "* IMDbデータセットの感情分析をPyTorchを使っておこなう。\n",
        " * \b前にscikit-learnを使って同じ作業をおこなった。\n",
        "* 参考資料\n",
        " * PyTorch公式のチュートリアル https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        "* データは以前作ったIMDbの文書埋め込みを使う。\n",
        "* sentiment analysisのもっと高度な手法については、下記リンク先を参照。\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-hkFIH1xHUX"
      },
      "source": [
        "## fastTextによる文書埋め込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0n2qW-6tsNC"
      },
      "source": [
        "### 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAv8yYt8-Yn0"
      },
      "source": [
        "* あらかじめランタイムのタイプをGPUに設定しておこう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf-jw4S8rhTB"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETpwV-Lmw-K5"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyTNFT_uZBVz"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 事前にfasttextでベクトル化されたIMDbデータを読み込む\n",
        " * 下記のリンク先にある`.npy`ファイルを、あらかじめ自分のGoogle Driveの適当な場所に置いておく。\n",
        "  * https://drive.google.com/drive/folders/1wSoIzSbZ2UqGQowiVDBI20h_A3hQNbtV?usp=sharing"
      ],
      "metadata": {
        "id": "XlWnT50NEQfD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjF1vzSosL5l"
      },
      "source": [
        "PATH = '/content/drive/MyDrive/2022Courses/nlp/imdb/'\n",
        "\n",
        "texts = dict()\n",
        "labels = dict()\n",
        "for tag in ['train', 'test']:\n",
        "  with open(f'{PATH}{tag}.npy', 'rb') as f:\n",
        "    texts[tag] = np.load(f)\n",
        "  with open(f'{PATH}{tag}_labels.npy', 'rb') as f:\n",
        "    labels[tag] = np.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE7RXSjEsWPU"
      },
      "source": [
        "for tag in ['train', 'test']:\n",
        "  print(texts[tag].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuEe746MtFtr"
      },
      "source": [
        "for tag in ['train', 'test']:\n",
        "  texts[tag], labels[tag] = torch.tensor(texts[tag]), torch.tensor(labels[tag])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0QVlHKJ-vZn"
      },
      "source": [
        "## データセットの準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du2OuR_1qeQi"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1osG09aHppg1"
      },
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.y[index]\n",
        "\n",
        "train_valid = MyDataset(texts['train'], labels['train'])\n",
        "test = MyDataset(texts['test'], labels['test'])\n",
        "\n",
        "valid_size = len(train_valid) // 5\n",
        "train_size = len(train_valid) - valid_size\n",
        "train, valid = random_split(train_valid,\n",
        "                            [train_size, valid_size],\n",
        "                            generator=torch.Generator().manual_seed(42)\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipq2udez-0ez"
      },
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfloEv0Osj2n"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ミニバッチのサイズ\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# 訓練データだけシャッフル\n",
        "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE-wx62k9d3J"
      },
      "source": [
        "## モデルの定義と学習の準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YDOQzmVr0N6"
      },
      "source": [
        "### モデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkRnetIrs9DX"
      },
      "source": [
        "class TextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class):\n",
        "    super(TextSentiment, self).__init__()\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLkGnlqfvQpp"
      },
      "source": [
        "EMBED_DIM = texts['train'].size(1)\n",
        "NUM_CLASS = len(np.unique(labels['train']))\n",
        "model = TextSentiment(EMBED_DIM, NUM_CLASS).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fqF_wxjZkPb"
      },
      "source": [
        "print(EMBED_DIM, NUM_CLASS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRWTIEYy_FKj"
      },
      "source": [
        "### 損失関数と最適化アルゴリズム"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVWxfoJSzpKp"
      },
      "source": [
        "* 損失関数を除いて、以下の設定はいい加減なので、自分で調整してみよう。\n",
        "* schedulerの使い方は、調べてみよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB0NIxhlxka5"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,50], gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W9fIHDa9j0P"
      },
      "source": [
        "## 分類器の訓練と評価"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0ARJYib_M84"
      },
      "source": [
        "### 評価を行なう関数\n",
        "* 正解率で評価する関数を定義しておく。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDFVMgRqyl4Q"
      },
      "source": [
        "def eval(model, criterion, loader):\n",
        "  model.eval()\n",
        "  \n",
        "  total_loss = 0.0\n",
        "  total_acc = 0.0\n",
        "  total_size = 0\n",
        "  for input, target in loader:\n",
        "    with torch.no_grad():\n",
        "      input, target = input.to(device), target.to(device)\n",
        "      output = model(input)\n",
        "      loss = criterion(output, target)\n",
        "      total_loss += loss.item() * len(target)\n",
        "      total_acc += (output.argmax(1) == target).sum().item()\n",
        "      total_size += len(target)\n",
        "\n",
        "  return total_loss / total_size, total_acc / total_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0ko1VZz_Kng"
      },
      "source": [
        "### 訓練を行なう関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHvS00cVvaVU"
      },
      "source": [
        "def train(model, criterion, optimizer, train_loader, valid_loader, n_epochs=100):\n",
        "  model.train()\n",
        "\n",
        "  # training loop\n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "    train_loss = 0.0\n",
        "    for input, target in train_loader:\n",
        "      output = model(input.to(device))\n",
        "      loss = criterion(output, target.to(device))\n",
        "      train_loss += loss.item() * len(target) # 表示用の集計\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    valid_loss, valid_acc = eval(model, criterion, valid_loader)\n",
        "\n",
        "    # logging\n",
        "    print(f'epoch {epoch + 1:6d} |',\n",
        "          f'train loss {train_loss / train_size:8.4f} |',\n",
        "          f'valid loss {valid_loss:8.4f} | valid acc {valid_acc:8.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEAiT90S4qae"
      },
      "source": [
        "### 訓練と評価の実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAXpaWNwxqWN"
      },
      "source": [
        "train(model, criterion, optimizer, train_loader, valid_loader, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* training lossとvalidation lossの差が大きいと、generalizeしない。\n",
        "* 以下、各自試行錯誤してください。"
      ],
      "metadata": {
        "id": "MVtds4jFFoss"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gLwC3XDdK3-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ハイパーパラメータのチューニングが済んだら、テストセットで評価する。"
      ],
      "metadata": {
        "id": "Mi5MZ1-GGcUA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4trquZpJGgop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbdnML0AuQv1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGDP7J3srA-d"
      },
      "source": [
        "## 単語埋め込みもパラメータになっているモデル\n",
        "* fasttextの単語埋め込みを使うのをやめる。\n",
        "* 単語埋め込みも同時に学習することにする。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM4Ku5bQrJlr"
      },
      "source": [
        "### IMDbデータセットをテキストデータとして読み直す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkj_D4IIeDvd"
      },
      "source": [
        "!pip install ml_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoXbgo88eFsb"
      },
      "source": [
        "from ml_datasets import imdb\n",
        "train_data, test_data = imdb()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saAM3Wp3eHg9"
      },
      "source": [
        "train_texts, train_labels = zip(*train_data)\n",
        "test_texts, test_labels = zip(*test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgmne7DYeKTv"
      },
      "source": [
        "train_texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcL7GnJ_kLSA"
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao2-FVU3rT04"
      },
      "source": [
        "### ラベルを0/1の整数に変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6WVH_sKkRfF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "unique_labels = np.unique(train_labels)\n",
        "label_id = {}\n",
        "for i, label in enumerate(unique_labels):\n",
        "  label_id[label] = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTfw1B-pkriP"
      },
      "source": [
        "train_labels = [label_id[label] for label in train_labels]\n",
        "test_labels = [label_id[label] for label in test_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU9OGXwEk4SX"
      },
      "source": [
        "print(train_labels[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bjc7f1mrYT5"
      },
      "source": [
        "### sklearnのCountVectorizerを使ってトークン化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbx6PzxvriMt"
      },
      "source": [
        "* 語彙集合の構築"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QywYL6Ise-5h"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=10, max_df=0.2)\n",
        "vectorizer.fit(train_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfDGUuTBf11l"
      },
      "source": [
        "vocab = vectorizer.get_feature_names_out()\n",
        "print([vocab[i] for i in range(10)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpVZZrqEvQfx"
      },
      "source": [
        "* ある単語が語彙集合に入っているかどうかは、下のようにしてチェックできる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGN_JN9KgSWt"
      },
      "source": [
        "'to' in vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7CSZkLpU-c2"
      },
      "source": [
        "'machine' in vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdzpT5R9rq5X"
      },
      "source": [
        "* preprocessorとtokenizerの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PETeskPfQXS"
      },
      "source": [
        "preprocessor = vectorizer.build_preprocessor()\n",
        "tokenizer = vectorizer.build_tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rGTz2nTrthv"
      },
      "source": [
        "* トークン列をインデックス列に変換する関数\n",
        " * 単語のインデックスを、パディング用の単語と、未知語との２つ分、後ろにずらす。\n",
        " * テキストの長さを`max_len`に揃えるという作業も同時に行なう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaIJVmCffXEW"
      },
      "source": [
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "VOCAB_SIZE = len(vocab) + 2\n",
        "\n",
        "def encode(text, max_len=1000, padding_idx=PAD_IDX, unknown_idx=UNK_IDX):\n",
        "  idx_seq = []\n",
        "  for token in tokenizer(preprocessor(text)):\n",
        "    if token in vectorizer.vocabulary_:\n",
        "      # PAD_IDX=0とUNK_IDX=1を追加したので、通常の単語のインデックスは2つ増やす\n",
        "      idx_seq.append(vectorizer.vocabulary_[token] + 2) \n",
        "    else:\n",
        "      idx_seq.append(unknown_idx)\n",
        "  if len(idx_seq) < max_len:\n",
        "    idx_seq += [padding_idx] * (max_len - len(idx_seq))\n",
        "  else:\n",
        "    idx_seq = idx_seq[:max_len]\n",
        "  return idx_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fYL82BDqNYm"
      },
      "source": [
        "print(VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSGwirhyhFfe"
      },
      "source": [
        "print(encode(train_texts[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPq9QZynr8ft"
      },
      "source": [
        "* バッチ単位でトークン列をインデックス列に変換する関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ZjuJcOl6pr"
      },
      "source": [
        "def batch_encode(texts):\n",
        "  sequences = []\n",
        "  for text in texts:\n",
        "    sequences.append(encode(text))\n",
        "  return torch.Tensor(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF5Vv9fCsEB3"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBmAulBKiwpl"
      },
      "source": [
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "class MyTextDataset(Dataset):\n",
        "  def __init__(self, texts, labels):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.texts[index], self.labels[index]\n",
        "\n",
        "train_valid_set = MyTextDataset(train_texts, train_labels)\n",
        "test_set = MyTextDataset(test_texts, test_labels)\n",
        "\n",
        "valid_size = len(train_valid_set) // 5\n",
        "train_size = len(train_valid_set) - valid_size\n",
        "train_set, valid_set = random_split(train_valid_set,\n",
        "                                    [train_size, valid_size],\n",
        "                                    generator=torch.Generator().manual_seed(42)\n",
        "                                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaWQOTx_sGIe"
      },
      "source": [
        "### DataLoader\n",
        "* ミニバッチのテキストをインデックス列へ変換するcollation用の関数も定義する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-rqRRyjmcmG"
      },
      "source": [
        "def collate_fn(batch):\n",
        "  batch_texts, batch_labels = zip(*batch)\n",
        "  batch_texts = batch_encode(batch_texts)\n",
        "  return batch_texts.type(torch.LongTensor), torch.LongTensor(batch_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAYc_ysrjbfP"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ミニバッチのサイズ\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# 訓練データだけシャッフル\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0x3kqRcsTb7"
      },
      "source": [
        "### モデルの定義\n",
        "* `nn.Embedding`を使う。\n",
        " * 語彙サイズ、埋め込みの次元数、パディング用の特殊なトークンのインデックスを指定する。\n",
        " * パディング用トークンのembeddingはゼロベクトルになる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbiMoa3adP7E"
      },
      "source": [
        "class EmbeddedTextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx=PAD_IDX):\n",
        "    super(EmbeddedTextSentiment, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, text):\n",
        "    embedded = self.dropout(self.embed(text))\n",
        "    x = embedded.mean(1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljnfpme3v8TF"
      },
      "source": [
        "* モデルのインスタンスを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE_sBuU2iSMC"
      },
      "source": [
        "EMBED_DIM = 300\n",
        "NUM_CLASS = len(np.unique(train_labels))\n",
        "model = EmbeddedTextSentiment(EMBED_DIM, NUM_CLASS, VOCAB_SIZE, PAD_IDX).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKWx0ApasYxc"
      },
      "source": [
        "### 損失関数と最適化アルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD61kYGMqa7I"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,50], gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPncv39nsb3s"
      },
      "source": [
        "### 学習の実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BPcXFmejU3x"
      },
      "source": [
        "train(model, criterion, optimizer, train_loader, valid_loader, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdegxekVwvCP"
      },
      "source": [
        "loss, acc = eval(model, criterion, train_loader)\n",
        "print(f'train loss {loss:8.4f} | train acc {acc:8.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAuufgZWW8tH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE7wz0tUW-1U"
      },
      "source": [
        "* 検証セット上での評価値でチューニングしてから、テストセットで最終評価。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "702pelIEW4QV"
      },
      "source": [
        "loss, acc = eval(model, criterion, test_loader)\n",
        "print(f'test loss {loss:8.4f} | test acc {acc:8.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUzVkJ2v-tt_"
      },
      "source": [
        "# 課題\n",
        "* モデルやoptimizerやschedulerを変更して、validation setを使ってチューニングしよう。\n",
        "* 最後に、自分で選択した設定を使って、test set上で評価しよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prYPP13ByCid"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIFqTNLYba53"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26RYXnmnbbSc"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnzANMG_vrT6"
      },
      "source": [
        "loss, acc = eval(model, criterion, test_loader)\n",
        "print(f'test loss {loss:8.4f} | test acc {acc:8.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}