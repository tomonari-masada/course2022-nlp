{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Hz9rXJHZDZeEY4pbWkLYFhZftVekADdY",
      "authorship_tag": "ABX9TyNb1iMdGQDwTnSNDJNkZDsO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2022-nlp/blob/main/04_word_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t-iVvqcVnB5"
      },
      "source": [
        "# **単語ベクトル**\n",
        "* 文書をbag-of-wordsモデルによってベクトル表現することは、最近はあまり行われない。\n",
        "* まず単語のベクトル表現を得て、それを使って文書のベクトル表現を得る、という手順をとる。\n",
        " * こうなったのは、[word2vec](https://arxiv.org/abs/1301.3781)と呼ばれる手法が登場して以降。\n",
        "  * https://en.wikipedia.org/wiki/Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3F0XNxABxH_"
      },
      "source": [
        "## spaCyの単語ベクトル\n",
        "* 今回は英語テキストのみ。\n",
        "* 小規模のモデル（名前が__`_sm`__で終わるモデル）は単語ベクトルを含まない。\n",
        "* 大規模モデルはダウンロードに時間がかかる。\n",
        "* そのため、中規模モデルをインストールする。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4qKeUfdCDNu"
      },
      "source": [
        "### spaCyの中規模モデルをダウンロード\n",
        "* https://spacy.io/models/en#en_core_web_md "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "-4jI7POINHwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MojyJdS4Cbbd"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXK0UdENE-jT"
      },
      "source": [
        "### テキストをtokenizeする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxAuMKmbVlnW"
      },
      "source": [
        "tokens = nlp(\"Dogs and cats have afskfsd\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2SzAM9aDvXP"
      },
      "source": [
        "for token in tokens:\n",
        "  print((f'単語:{token.text}, ベクトルの有無:{token.has_vector},'\n",
        "        f'ベクトルのL2ノルム:{token.vector_norm:.4f}, \bベクトルがOoVか否か:{token.is_oov}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di8hwjB9FUhg"
      },
      "source": [
        "* 単語ベクトルの型と中身を確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gY23wgHEkij"
      },
      "source": [
        "type(tokens[0].vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gb5jxhID0S1"
      },
      "source": [
        "tokens[0].vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZt6FgxJngBB"
      },
      "source": [
        "tokens[0].vector.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg0pELuZFe0H"
      },
      "source": [
        "* OoV(Out of Vocabulary)の単語ベクトルはゼロベクトル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhObUeE0Eg9H"
      },
      "source": [
        "print(f'{tokens[-1].text}\\n{tokens[-1].vector}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_z-hDrkoE-G"
      },
      "source": [
        "### トークン列のベクトル表現を得る"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxwRSbPwoCPq"
      },
      "source": [
        "tokens.vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MkGyUTgWwT1"
      },
      "source": [
        "* トークン列のベクトルを求めると、OoVのゼロベクトルも含めて平均が計算されるようだ。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvd4L57pwmpa"
      },
      "source": [
        "import numpy as np\n",
        "np.allclose(tokens[:-1].vector * (len(tokens) - 1), tokens.vector * len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9cxfLvGGYLY"
      },
      "source": [
        "* 多義語であっても単語ベクトルはひとつだけ\n",
        " * 意味の数だけ別々のベクトルが用意されていたりはしない。\n",
        "* 例：社名のアップルであろうと、りんごのアップルであろうと、単語ベクトルは同一\n",
        " * `similarity`メソッドで、単語ベクトルどうしのコサイン類似度を計算できる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7i_BHd5FaTa"
      },
      "source": [
        "doc = nlp(\"Apple shares rose on the news. Apple pie is delicious.\")\n",
        "print(doc[0].similarity(doc[7]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7uGjrrAIlvV"
      },
      "source": [
        "### 文書類似度の計算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F63Ofq-MIpAB"
      },
      "source": [
        "doc1 = nlp(\"It's a warm summer day\")\n",
        "doc2 = nlp(\"It's sunny outside\")\n",
        "doc3 = nlp(\"It's definitely cold outside\")\n",
        "\n",
        "print(doc1.similarity(doc2))\n",
        "print(doc1.similarity(doc3))\n",
        "print(doc2.similarity(doc3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EIlDmklyYaP"
      },
      "source": [
        "# 単語ベクトルを使った文書分類\n",
        "* 文書に含まれる単語の単語ベクトルから文書のベクトル表現を得る。\n",
        "* 文書のベクトル表現を使って2値分類問題を解く。\n",
        "* sentiment analysisの有名なデータセットであるIMDbを使う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OMyhtDIbg0r"
      },
      "source": [
        "### IMDbデータセット\n",
        "\n",
        "* データセットの基本情報\n",
        " * Webサイト: https://ai.stanford.edu/~amaas/data/sentiment/\n",
        " * 作成者: Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng and Christopher Potts\n",
        " * タイトル: Large Movie Review Dataset (aka. IMDb Review Dataset)\n",
        " * 公開日: Jun, 2011"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_qATgIcyxlk"
      },
      "source": [
        "* データのロード\n",
        " * ml-datasetsというツールを使う。\n",
        " * https://pypi.org/project/ml-datasets/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gauCQ6pq63d"
      },
      "source": [
        "!pip install ml_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE3YO-pyq35J"
      },
      "source": [
        "from ml_datasets import imdb\n",
        "train_data, test_data = imdb()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnceAxpVG8s2"
      },
      "source": [
        "print(f'{len(train_data)} {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6-_KL5z28ce"
      },
      "source": [
        "train_texts, train_labels = zip(*train_data)\n",
        "test_texts, test_labels = zip(*test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlQ4ISYZ4Txs"
      },
      "source": [
        "train_texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjIIZ7YQ5Vaf"
      },
      "source": [
        "print(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAghTle4s8sd"
      },
      "source": [
        "* spaCyを使ってIMDbデータセットの全文書をベクトル化してみる\n",
        " * 1,000件でも結構時間がかかる…。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyynbCYlsjf9"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "X_train = []\n",
        "for text in tqdm(train_texts[:1000]):\n",
        "  X_train.append(nlp(text).vector)\n",
        "X_train = np.array(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTFG6tRlGQYY"
      },
      "source": [
        "## fasttextの単語ベクトルを使う\n",
        "* spaCyの単語ベクトルは、Pythonで実装されているので、遅い。\n",
        "* fasttextは、C++で実装されているので、速い。\n",
        " * しかし、単語ベクトルデータのサイズが巨大なので、Google Colab向きではない。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrjq9WQT_evz"
      },
      "source": [
        "### fasttextのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTn2oj1NGuW7"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0HhBNNX_lSr"
      },
      "source": [
        "### 言語モデルのダウンロード\n",
        "* 7GB強のサイズがあるので、非常に時間がかかる。\n",
        " * こういう作業はGoogle Colabでは行わないほうが良いかも。\n",
        "* 今回は、諦める。（試したい方は試してください。）\n",
        "* 手元の環境で実行する際の手順だけを、以下のセルに示す。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoN3GLJVGa_A"
      },
      "source": [
        "import fasttext.util\n",
        "fasttext.util.download_model('en', if_exists='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QE2IOVRqTP1"
      },
      "source": [
        "### 文書のベクトル化\n",
        "* モデルをロードしてから、ベクトル化。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNbYKpQhGm3R"
      },
      "source": [
        "model = fasttext.load_model('cc.en.300.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5763OiNqKsD"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train = []\n",
        "for text in train_texts:\n",
        "  X_train.append(model.get_sentence_vector(text.replace(\"\\n\",\" \")))\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "X_test = []\n",
        "for text in test_texts:\n",
        "  X_test.append(model.get_sentence_vector(text.replace(\"\\n\",\" \")))\n",
        "X_test = np.array(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0uU0v1CAEpj"
      },
      "source": [
        "* ベクトル化した結果をファイルとして保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs37au0nqij5"
      },
      "source": [
        "with open('train.npy', 'wb') as f:\n",
        "  np.save(f, X_train)\n",
        "with open('test.npy', 'wb') as f:\n",
        "  np.save(f, X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kok7PUdvrT_-"
      },
      "source": [
        "with open('train_labels.npy', 'wb') as f:\n",
        "  np.save(f, np.array(train_labels))\n",
        "with open('test_labels.npy', 'wb') as f:\n",
        "  np.save(f, np.array(test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4_RQVCxksgG"
      },
      "source": [
        "## 事前にfasttextでベクトル化されたIMDbデータをロード\n",
        " * 下記のリンク先にある`.npy`ファイルを、あらかじめ自分のGoogle Driveの適当な場所に置いておく。\n",
        "  * https://drive.google.com/drive/folders/1wSoIzSbZ2UqGQowiVDBI20h_A3hQNbtV?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7A-j1BDeN6f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "PATH = '/content/drive/MyDrive/2022Courses/nlp/imdb/'\n",
        "texts = dict()\n",
        "labels = dict()\n",
        "for tag in ['train', 'test']:\n",
        "  with open(f'{PATH}{tag}.npy', 'rb') as f:\n",
        "    texts[tag] = np.load(f)\n",
        "  with open(f'{PATH}{tag}_labels.npy', 'rb') as f:\n",
        "    labels[tag] = np.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtFQvkcOrb6x"
      },
      "source": [
        "texts['train'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im8djYmZkB3b"
      },
      "source": [
        "## 事前学習済みTransformerで文書をベクトル化\n",
        "* Transformerの説明はしない。とりあえず使う。\n",
        "* Transformerを単なるエンコーダとして使う。\n",
        " * fine tuningはしない。\n",
        "* 今回は、sentence transformersを使う。\n",
        " * https://github.com/UKPLab/sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6V_dyqv0n1o"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoJVdkZQk801"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LalaUJ22k9dU"
      },
      "source": [
        "apple1 = model.encode(\"Apple shares rose on the news.\")\n",
        "apple2 = model.encode(\"Apple sold fewer iPhones this quarter.\")\n",
        "apple3 = model.encode(\"Apple pie is delicious.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IigWw8LpWE-"
      },
      "source": [
        "apple1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LirtdFrfpZLk"
      },
      "source": [
        "apple1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPaURBeH1D4r"
      },
      "source": [
        "from sentence_transformers import util\n",
        "\n",
        "print(util.cos_sim(apple1, apple2))\n",
        "print(util.cos_sim(apple1, apple3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBiyaMmB-5oZ"
      },
      "source": [
        "* IMDbの全文書のベクトル化には、やはりそれなりに時間がかかる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoOyuhcGmwN8"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "X_train = []\n",
        "for text in tqdm.tqdm(train_texts[:100]):\n",
        "  doc = model.encode(text)\n",
        "  X_train.append(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss1gBFpoATIR"
      },
      "source": [
        "# 課題3\n",
        "* 春学期に習った分類手法を使って、IMDbデータセットの感情分析をしてみよう。\n",
        " * training set / test setの分割は、そのまま使う。\n",
        " * training setをどのように使うかはお任せします。（交差検証など。）\n",
        " * test setでの分類性能をArea under the ROC curveで報告。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnK61odfnND7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}